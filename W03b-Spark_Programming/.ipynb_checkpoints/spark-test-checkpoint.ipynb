{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "486b40ec",
   "metadata": {},
   "source": [
    "# CS5052 - Spark Programming\n",
    "> Created by: Professor Blesson Varghese\\\n",
    "> School of Computer Science, University of St Andrews\\\n",
    "> Contact: cs5052.staff@st-andrews.ac.uk\n",
    "\n",
    "This notebook introduces you to Spark programming using Python. Spark is a system that coordinates the processing of large datasets in parallel across many machines. In practice, you could run Spark across a cluster of nodes that will be managed by Spark. Spark in the context of the lab is installed and run on a single machine. \n",
    "\n",
    "You can setup the enviroment to run this notebook on the lab machine by: \n",
    "```\n",
    "cd <your desired folder>\n",
    "python3.12 -m venv pyspark\n",
    ". pyspark/bin/activate\n",
    "pip install --upgrade pip\n",
    "pip install pyspark jupyterlab\n",
    "```\n",
    "\n",
    "Run the JupyterLab server after activating the virtual environment using the following command:\n",
    "```\n",
    "jupyter-lab\n",
    "```\n",
    "A browser window should open automatically.\n",
    "\n",
    "To create self-contained notebooks, explicit commands must be provided in the code within the notebook for installing any additional packages using the following command:\n",
    "```\n",
    "%pip install <package_name>\n",
    "```\n",
    "\n",
    "**Note:** The notebook submitted for the CS5052 Practical 1 must run on the lab machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2e1dac",
   "metadata": {},
   "source": [
    "# `SparkSession`\n",
    "\n",
    "- Every Spark application consists of a driver program and executors (workers); see figure below\n",
    "- Driver program accesses Spark through a `SparkSession` object\n",
    "    - A unified point of entry as of Spark 2.0\n",
    "    - Represents a connection to a cluster\n",
    "    - `SparkContext`, `SQLContext` and `HiveContext` all combined in `SparkSession`\n",
    "\n",
    " ![Spark Overview; Obtained from: https://spark.apache.org/docs/latest/cluster-overview.html](images/sparksession.png)"
   ]
  },
  {
   "cell_type": "code",
   "id": "369588b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:31:25.656037Z",
     "start_time": "2026-02-13T19:31:25.046994Z"
    }
   },
   "source": [
    "# Import SparkSession class from pyspark.sql module\n",
    "# SparkSession is the entry point to Spark \n",
    "from pyspark.sql import SparkSession"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "code() argument 13 must be str, not int",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Import SparkSession class from pyspark.sql module\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;66;03m# SparkSession is the entry point to Spark \u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpyspark\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01msql\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SparkSession\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/lib/python3.14/site-packages/pyspark/__init__.py:51\u001B[39m\n\u001B[32m     48\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtypes\u001B[39;00m\n\u001B[32m     50\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpyspark\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mconf\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SparkConf\n\u001B[32m---> \u001B[39m\u001B[32m51\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpyspark\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcontext\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SparkContext\n\u001B[32m     52\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpyspark\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mrdd\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m RDD, RDDBarrier\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpyspark\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mfiles\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SparkFiles\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/lib/python3.14/site-packages/pyspark/context.py:30\u001B[39m\n\u001B[32m     27\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpy4j\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mprotocol\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Py4JError\n\u001B[32m     28\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpy4j\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mjava_gateway\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m is_instance_of\n\u001B[32m---> \u001B[39m\u001B[32m30\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpyspark\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m accumulators\n\u001B[32m     31\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpyspark\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01maccumulators\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Accumulator\n\u001B[32m     32\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpyspark\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mbroadcast\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Broadcast, BroadcastPickleRegistry\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/lib/python3.14/site-packages/pyspark/accumulators.py:97\u001B[39m\n\u001B[32m     95\u001B[39m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msocketserver\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mSocketServer\u001B[39;00m\n\u001B[32m     96\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mthreading\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m97\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpyspark\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mserializers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m read_int, PickleSerializer\n\u001B[32m    100\u001B[39m __all__ = [\u001B[33m'\u001B[39m\u001B[33mAccumulator\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mAccumulatorParam\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m    103\u001B[39m pickleSer = PickleSerializer()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/lib/python3.14/site-packages/pyspark/serializers.py:71\u001B[39m\n\u001B[32m     68\u001B[39m     xrange = \u001B[38;5;28mrange\u001B[39m\n\u001B[32m     69\u001B[39m pickle_protocol = pickle.HIGHEST_PROTOCOL\n\u001B[32m---> \u001B[39m\u001B[32m71\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpyspark\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m cloudpickle\n\u001B[32m     72\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpyspark\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutil\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _exception_message, print_exec\n\u001B[32m     75\u001B[39m __all__ = [\u001B[33m\"\u001B[39m\u001B[33mPickleSerializer\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mMarshalSerializer\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mUTF8Deserializer\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/lib/python3.14/site-packages/pyspark/cloudpickle.py:209\u001B[39m\n\u001B[32m    190\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    191\u001B[39m             \u001B[38;5;28;01mreturn\u001B[39;00m types.CodeType(\n\u001B[32m    192\u001B[39m                 co.co_argcount,\n\u001B[32m    193\u001B[39m                 co.co_kwonlyargcount,\n\u001B[32m   (...)\u001B[39m\u001B[32m    206\u001B[39m                 (),\n\u001B[32m    207\u001B[39m             )\n\u001B[32m--> \u001B[39m\u001B[32m209\u001B[39m _cell_set_template_code = \u001B[43m_make_cell_set_template_code\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    212\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcell_set\u001B[39m(cell, value):\n\u001B[32m    213\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Set the value of a closure cell.\u001B[39;00m\n\u001B[32m    214\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/lib/python3.14/site-packages/pyspark/cloudpickle.py:172\u001B[39m, in \u001B[36m_make_cell_set_template_code\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    170\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    171\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(types.CodeType, \u001B[33m\"\u001B[39m\u001B[33mco_posonlyargcount\u001B[39m\u001B[33m\"\u001B[39m):  \u001B[38;5;66;03m# pragma: no branch\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m172\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtypes\u001B[49m\u001B[43m.\u001B[49m\u001B[43mCodeType\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    173\u001B[39m \u001B[43m            \u001B[49m\u001B[43mco\u001B[49m\u001B[43m.\u001B[49m\u001B[43mco_argcount\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    174\u001B[39m \u001B[43m            \u001B[49m\u001B[43mco\u001B[49m\u001B[43m.\u001B[49m\u001B[43mco_posonlyargcount\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Python3.8 with PEP570\u001B[39;49;00m\n\u001B[32m    175\u001B[39m \u001B[43m            \u001B[49m\u001B[43mco\u001B[49m\u001B[43m.\u001B[49m\u001B[43mco_kwonlyargcount\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    176\u001B[39m \u001B[43m            \u001B[49m\u001B[43mco\u001B[49m\u001B[43m.\u001B[49m\u001B[43mco_nlocals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    177\u001B[39m \u001B[43m            \u001B[49m\u001B[43mco\u001B[49m\u001B[43m.\u001B[49m\u001B[43mco_stacksize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    178\u001B[39m \u001B[43m            \u001B[49m\u001B[43mco\u001B[49m\u001B[43m.\u001B[49m\u001B[43mco_flags\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    179\u001B[39m \u001B[43m            \u001B[49m\u001B[43mco\u001B[49m\u001B[43m.\u001B[49m\u001B[43mco_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    180\u001B[39m \u001B[43m            \u001B[49m\u001B[43mco\u001B[49m\u001B[43m.\u001B[49m\u001B[43mco_consts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    181\u001B[39m \u001B[43m            \u001B[49m\u001B[43mco\u001B[49m\u001B[43m.\u001B[49m\u001B[43mco_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    182\u001B[39m \u001B[43m            \u001B[49m\u001B[43mco\u001B[49m\u001B[43m.\u001B[49m\u001B[43mco_varnames\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    183\u001B[39m \u001B[43m            \u001B[49m\u001B[43mco\u001B[49m\u001B[43m.\u001B[49m\u001B[43mco_filename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    184\u001B[39m \u001B[43m            \u001B[49m\u001B[43mco\u001B[49m\u001B[43m.\u001B[49m\u001B[43mco_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    185\u001B[39m \u001B[43m            \u001B[49m\u001B[43mco\u001B[49m\u001B[43m.\u001B[49m\u001B[43mco_firstlineno\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    186\u001B[39m \u001B[43m            \u001B[49m\u001B[43mco\u001B[49m\u001B[43m.\u001B[49m\u001B[43mco_lnotab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    187\u001B[39m \u001B[43m            \u001B[49m\u001B[43mco\u001B[49m\u001B[43m.\u001B[49m\u001B[43mco_cellvars\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# this is the trickery\u001B[39;49;00m\n\u001B[32m    188\u001B[39m \u001B[43m            \u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    189\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    190\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    191\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m types.CodeType(\n\u001B[32m    192\u001B[39m             co.co_argcount,\n\u001B[32m    193\u001B[39m             co.co_kwonlyargcount,\n\u001B[32m   (...)\u001B[39m\u001B[32m    206\u001B[39m             (),\n\u001B[32m    207\u001B[39m         )\n",
      "\u001B[31mTypeError\u001B[39m: code() argument 13 must be str, not int"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c306afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession and assign it to variable 'spark'\n",
    "# There are different variants on the usage - refer to the documentation or a tutorial\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce864ecb",
   "metadata": {},
   "source": [
    "# DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56609aa0",
   "metadata": {},
   "source": [
    "## DataFrame: create manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974011f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with one column called “number” and 10000 rows\n",
    "data = spark.range(1000).toDF(\"number\")\n",
    "\n",
    "# Shows the first 20 rows by default\n",
    "data.show()\n",
    "\n",
    "# Show more or fewer rows N\n",
    "N = 50\n",
    "data.show(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ac6cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Python list containing two rows\n",
    "emp = [Row(\"Jack\", 24), Row(\"Bobby\", 26)]\n",
    "\n",
    "# Convert Python data into Spark DataFrame\n",
    "emp_df = spark.createDataFrame(emp, [\"name\",\"age\"])\n",
    "\n",
    "emp_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7750b44d",
   "metadata": {},
   "source": [
    "## DataFrame: create from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4cf528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from a file\n",
    "df = ( \n",
    "    spark.read\n",
    "    .option(\"header\", True)         # Tells Spark the first line is a header\n",
    "    .option(\"inferSchema\", True)    # Spark scans the column and infers data type; id will be an integer, country and capital will be a string\n",
    "    .format(\"csv\")                  \n",
    "    .load(\"sample_data1.csv\")\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e8fe5a",
   "metadata": {},
   "source": [
    "## DataFrame: Datasource\n",
    "\n",
    "Many different file types are possible, including CSV, JSON, ORC, Parquet, Text, Table, JDBC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef0ceb4",
   "metadata": {},
   "source": [
    "# Two Major Operations\n",
    "\n",
    "- All abstractions such as RDD and DataFrames offer two types of operation\n",
    "    - **Transformation:** construct a new RDD/DataFrame from a previous one\n",
    "    - **Action:** compute the result based on an RDD/DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93597aad",
   "metadata": {},
   "source": [
    "# Transformations\n",
    "\n",
    "## Transformations: `printSchema()` and `describe()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d12110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the structure (data type of the columns) of the DataFrame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee06ab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describes the schema of the DataFrame\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4517cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the structure of specific column\n",
    "df.select(\"Country\").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dc155b",
   "metadata": {},
   "source": [
    "## Transformations: `where()` and `filter()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c958ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_population = ( \n",
    "                    spark.read\n",
    "                    .option(\"header\", True)         \n",
    "                    .option(\"inferSchema\", True) \n",
    "                    .format(\"csv\")                  \n",
    "                    .load(\"sample_data2.csv\")\n",
    ")\n",
    "\n",
    "df_population.show()\n",
    "\n",
    "hundredK_plus = df_population.filter(\"Population >= 100000\")\n",
    "hundredK_plus.show()\n",
    "\n",
    "under_50K = df_population.where(\"Population <= 50000\")\n",
    "under_50K.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce951e13",
   "metadata": {},
   "source": [
    "## Transformation: `distinct()` and\t`limit()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95a04f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_town_village = ( \n",
    "                    spark.read\n",
    "                    .option(\"header\", True)         \n",
    "                    .option(\"inferSchema\", True) \n",
    "                    .format(\"csv\")                  \n",
    "                    .load(\"sample_data3.csv\")\n",
    ")\n",
    "\n",
    "df_town_village.show()\n",
    "\n",
    "unique_county = df_town_village.select(\"County\").distinct()\n",
    "unique_county.show()\n",
    "\n",
    "N = 5\n",
    "shortN_list = df_town_village.limit(N)\n",
    "shortN_list.show()\n",
    "\n",
    "# Alternate usage\n",
    "df_town_village.limit(N).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a3dc99",
   "metadata": {},
   "source": [
    "## Transformation: Sorting using `sort()` or `orderBy()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992b324c",
   "metadata": {},
   "source": [
    "### Basic sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab48fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by a single column\n",
    "sorted = df_town_village.sort(\"County\")\n",
    "sorted.show()\n",
    "\n",
    "# Sort by multiple columns\n",
    "sorted = df_town_village.sort(\"County\", \"Town/Village\")\n",
    "sorted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815e9fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order by a single column\n",
    "sorted = df_town_village.orderBy(\"Town/Village\")\n",
    "sorted.show()\n",
    "\n",
    "# Order by multiple columns\n",
    "sorted = df_town_village.orderBy(\"County\", \"Town/Village\")\n",
    "sorted.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e35e3a3",
   "metadata": {},
   "source": [
    "### Specifying sort direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafa0363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, asc \n",
    "\n",
    "sorted = df_town_village.orderBy(desc(\"Town/Village\"))\n",
    "sorted.show()\n",
    "\n",
    "sorted = df_town_village.orderBy(asc(\"County\"), desc(\"Town/Village\"))\n",
    "sorted.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170dd105",
   "metadata": {},
   "source": [
    "## Transformation: Sampling data using `sample`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bef7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_replacement = False    # Sample without replacement; each row can appear at most once \n",
    "fraction = 0.50             # Roughly 50% of the rows are selected\n",
    "seed = None                 # Sets the random seed for reproducibility; if an integer sample value is set it produces the same sample everytime \n",
    "\n",
    "sample = df_town_village.sample(with_replacement, fraction, seed)\n",
    "sample.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738e0474",
   "metadata": {},
   "source": [
    "## Transformation: Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d49bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, countDistinct \n",
    "\n",
    "df_town_village.select(count(\"County\")).show()\n",
    "\n",
    "df_town_village.select(countDistinct(\"County\")).show()\n",
    "\n",
    "# min, max, avg, first, last and groupBy functions are available and self explanatory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7114d6da",
   "metadata": {},
   "source": [
    "## DataFrame: Some Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3458c7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first()\n",
    "row = df_town_village.first()\n",
    "print(row)\n",
    "print(row[\"Town/Village\"])      #Access column of the first row\n",
    "\n",
    "# show()\n",
    "df_town_village.show()\n",
    "N = 6\n",
    "df_town_village.show(N)\n",
    "\n",
    "# take(N)\n",
    "N = 4\n",
    "rows = df_town_village.take(N)  #Similar to first, but returns multiple rows\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# collect()\n",
    "all_rows = df_town_village.collect()    #Returns all rows as a list of objects\n",
    "# Note: if the DataFrame is large, then may not work as all memory is brought into memory\n",
    "# Use this for small datasets or debugging\n",
    "for row in all_rows:\n",
    "    print(row)\n",
    "\n",
    "#count()\n",
    "print(f\"Total rows: {df_town_village.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8861f3f1",
   "metadata": {},
   "source": [
    "# RDD\n",
    "\n",
    "- Low level but still relevant in some cases:\n",
    "- Raw data processing e.g. text file without structure.\n",
    "    - Creating new RDDs\n",
    "    - Transforming existing RDDs\n",
    "    - Computing results from RDDs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fe500b",
   "metadata": {},
   "source": [
    "## Create RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846fcfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From an existing file\n",
    "lines = spark.sparkContext.textFile(\"README_dummy.md\")\n",
    "\n",
    "# or\n",
    "sc = spark.sparkContext\n",
    "lines = sc.textFile(\"README_dummy.md\")\n",
    "\n",
    "# Collect all lines into a Python list\n",
    "all_lines = lines.collect()\n",
    "\n",
    "# Print each line\n",
    "for line in all_lines:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a299b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a list\n",
    "numbers = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "# numbers is an RDD containing numbers 1 to 8\n",
    "# The data is split into partitions and can be processed in parallel\n",
    "\n",
    "# Aggregate the partitions and print all numbers\n",
    "print(numbers.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f78071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD from a file \n",
    "lines = sc.textFile(\"README_dummy.md\")\n",
    "\n",
    "# Create new RDD with lines containing Spark\n",
    "lines = lines.filter(lambda x: 'Spark' in x)\n",
    "\n",
    "# Count the number of items in this RDD\n",
    "# Note: The above two lines doesn't do anything \n",
    "# The statement below will read the file and do the computation\n",
    "print(lines.count())\n",
    "\n",
    "# The statement below will read the file and do the computation again\n",
    "print(lines.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200690c4",
   "metadata": {},
   "source": [
    "## RDD - Persisting\n",
    "\n",
    "- Spark recomputes RDDs each time an action is performed on it\n",
    "    - By default RDD is not stored in memory\n",
    "    - The `persist()` function stores an RDD permanently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d479898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"README_dummy.md\")\n",
    "lines = lines.filter(lambda x: 'Spark' in x)\n",
    "\n",
    "# Load and store dataset in memory\n",
    "lines.persist()\n",
    "\n",
    "# Perform computation on the stored dataset\n",
    "print(lines.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cb9faf",
   "metadata": {},
   "source": [
    "## Basic RDD Transformation Functions\n",
    "\n",
    "- Construct an RDD from a previous one\n",
    "    - Performed on one or more RDDs\n",
    "    - Return a new RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74bfbfc",
   "metadata": {},
   "source": [
    "### `filter()`\n",
    "- Takes in a function, returns an RDD that only has elements that pass the filter() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a6e708",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"README_dummy.md\")\n",
    "\n",
    "# Create a new RDD consisting lines that contain ‘Spark’\n",
    "lines = lines.filter(lambda x: 'Spark' in x)\n",
    "\n",
    "all_lines = lines.collect()\n",
    "\n",
    "for line in all_lines:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c319b1",
   "metadata": {},
   "source": [
    "### `map()`\n",
    "- Takes in a function and applies it to each element in the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b415d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"README_dummy.md\")\n",
    "\n",
    "# Create a new RDD in which all strings are in uppercase\n",
    "lines = lines.map(lambda x: x.upper())\n",
    "\n",
    "all_lines = lines.collect()\n",
    "\n",
    "for line in all_lines:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99285319",
   "metadata": {},
   "source": [
    "### `flatmap()`\n",
    "- Applies a function to each element in an RDD\n",
    "- Returns a sequence (list of elements)\n",
    "- The final RDD is flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cbc518",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.parallelize([\n",
    "    \"I love Spark\",\n",
    "    \"Spark is awesome\",\n",
    "    \"Big data rocks\"\n",
    "])\n",
    "\n",
    "words_using_map = lines.map(lambda line: line.split(\" \"))\n",
    "print(words_using_map.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26194aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_using_flatmap = lines.flatMap(lambda line: line.split(\" \"))\n",
    "print(words_using_flatmap.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03683f6",
   "metadata": {},
   "source": [
    "### `distinct()`\n",
    "- Returns a new RDD with only distinct items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21054f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = sc.parallelize([0, 1, 2, 4, 7, 5, 4, 3, 2, 1, 1, 0])\n",
    "numbers = numbers.distinct()\n",
    "print(numbers.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9720738",
   "metadata": {},
   "source": [
    "### `union(other)`\n",
    "- Returns a new RDD consisting of items from both sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3196c6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = sc.parallelize([0, 1, 2, 3, 4])\n",
    "characters = sc.parallelize(['A', 'B', 'C', 'D', 'E'])\n",
    "result = numbers.union(characters)\n",
    "print(result.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22e8ea3",
   "metadata": {},
   "source": [
    "### `intersection(other)`\n",
    "- Returns a new RDD consisting of only items from both sources and removes all duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6bc156",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_list1 = sc.parallelize([0, 1, 2, 4, 6, 7, 8])\n",
    "number_list2 = sc.parallelize([0, 1, 3, 4, 5, 7])\n",
    "result = number_list1.intersection(number_list2)\n",
    "print(result.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14cf11e",
   "metadata": {},
   "source": [
    "### `subtract(other)`\n",
    "- Returns a new RDD consisting of only items in the first RDD but not in the other one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb18594",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_list1 = sc.parallelize([0, 1, 2, 4, 6, 7, 8])\n",
    "number_list2 = sc.parallelize([0, 1, 3, 4, 5, 7])\n",
    "result = number_list1.subtract(number_list2)\n",
    "print(result.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f688102",
   "metadata": {},
   "source": [
    "## Basic RDD Action Functions\n",
    "\n",
    "- Compute result based on RDD(s)\n",
    "    - Performed on one or more RDD(s)\n",
    "    - Return a result, which is not an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aaf4e5",
   "metadata": {},
   "source": [
    "### `first()`\n",
    "- Returns the first item in an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8ac995",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = sc.parallelize([0, 1, 2, 3, 4, 5, 6, 7, 8]) \n",
    "print(numbers.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b64e2b3",
   "metadata": {},
   "source": [
    "### `collect()`\n",
    "- Returns a list containing the entire RDD's content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be6d975",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = sc.parallelize([0, 1, 2, 3, 4, 5, 6, 7, 8]) \n",
    "print(numbers.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c7cdb2",
   "metadata": {},
   "source": [
    "### `count()`\n",
    "- Returns the number of items in an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86dcef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = sc.parallelize([0, 1, 2, 3, 4, 5, 6, 7, 8]) \n",
    "print(numbers.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47196a9e",
   "metadata": {},
   "source": [
    "### `reduce(function)`\n",
    "- Takes a function that operates on two elements and returns a new element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9548b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = sc.parallelize([1, 2, 3, 4, 5]) \n",
    "result = numbers.reduce(lambda x, y: x * y)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be774f67",
   "metadata": {},
   "source": [
    "### `takeOrdered(num, ordering)`\n",
    "- Return a number of items based on the provided ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08f1183",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = sc.parallelize([8, 0, 4, 6, 9, 7, 2, 1, 5, 3])\n",
    "\n",
    "# Return five smallest numbers from the list\n",
    "print(numbers.takeOrdered(5, lambda x: x))\n",
    "\n",
    "# Return five largest numbers from the list\n",
    "print(numbers.takeOrdered(5, lambda x: -x))\n",
    "# Note: How this function works:\n",
    "# Original numbers: 8, 0, 4, 6, 9, 7, 2, 1, 5, 3\n",
    "# Negated numbers: -8, 0, -4, -6, -9, -7, -2, -1, -5, -3\n",
    "# 5 smallest of these: -9, -8, -7, -6, -5\n",
    "# Negate back: 9, 8, 7, 6, 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
