{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "486b40ec",
   "metadata": {},
   "source": [
    "# CS5052 - Spark Programming\n",
    "> Created by: Professor Blesson Varghese\\\n",
    "> School of Computer Science, University of St Andrews\\\n",
    "> Contact: cs5052.staff@st-andrews.ac.uk\n",
    "\n",
    "This notebook introduces you to Spark programming using Python. Spark is a system that coordinates the processing of large datasets in parallel across many machines. In practice, you could run Spark across a cluster of nodes that will be managed by Spark. Spark in the context of the lab is installed and run on a single machine. \n",
    "\n",
    "You can setup the enviroment to run this notebook on the lab machine by: \n",
    "```\n",
    "cd <your desired folder>\n",
    "python3.12 -m venv pyspark\n",
    ". pyspark/bin/activate\n",
    "pip install --upgrade pip\n",
    "pip install pyspark jupyterlab\n",
    "```\n",
    "\n",
    "Run the JupyterLab server after activating the virtual environment using the following command:\n",
    "```\n",
    "jupyter-lab\n",
    "```\n",
    "A browser window should open automatically.\n",
    "\n",
    "To create self-contained notebooks, explicit commands must be provided in the code within the notebook for installing any additional packages using the following command:\n",
    "```\n",
    "%pip install <package_name>\n",
    "```\n",
    "\n",
    "**Note:** The notebook submitted for the CS5052 Practical 1 must run on the lab machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2e1dac",
   "metadata": {},
   "source": [
    "# `SparkSession`\n",
    "\n",
    "- Every Spark application consists of a driver program and executors (workers); see figure below\n",
    "- Driver program accesses Spark through a `SparkSession` object\n",
    "    - A unified point of entry as of Spark 2.0\n",
    "    - Represents a connection to a cluster\n",
    "    - `SparkContext`, `SQLContext` and `HiveContext` all combined in `SparkSession`\n",
    "\n",
    " ![Spark Overview; Obtained from: https://spark.apache.org/docs/latest/cluster-overview.html](images/sparksession.png)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:19:05.651056700Z",
     "start_time": "2026-02-13T21:19:05.604631700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"Python being used:\", sys.executable)\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
   ],
   "id": "ee2c46e925ab0a4b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python being used: C:\\Users\\kverm\\PycharmProjects\\cs5052-practical1\\.venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "369588b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:19:05.684887900Z",
     "start_time": "2026-02-13T21:19:05.656055900Z"
    }
   },
   "source": [
    "# Import SparkSession class from pyspark.sql module\n",
    "# SparkSession is the entry point to Spark \n",
    "from pyspark.sql import SparkSession"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "c306afb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:19:05.720827800Z",
     "start_time": "2026-02-13T21:19:05.687886500Z"
    }
   },
   "source": [
    "# Create a SparkSession and assign it to variable 'spark'\n",
    "# There are different variants on the usage - refer to the documentation or a tutorial\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"test\") \\\n",
    "    .getOrCreate()\n"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "ce864ecb",
   "metadata": {},
   "source": [
    "# DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56609aa0",
   "metadata": {},
   "source": [
    "## DataFrame: create manually"
   ]
  },
  {
   "cell_type": "code",
   "id": "974011f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:19:05.959859Z",
     "start_time": "2026-02-13T21:19:05.725055Z"
    }
   },
   "source": [
    "# Create a DataFrame with one column called “number” and 10000 rows\n",
    "data = spark.range(1000).toDF(\"number\")\n",
    "\n",
    "# Shows the first 20 rows by default\n",
    "data.show()\n",
    "\n",
    "# Show more or fewer rows N\n",
    "N = 50\n",
    "data.show(N)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "|    10|\n",
      "|    11|\n",
      "|    12|\n",
      "|    13|\n",
      "|    14|\n",
      "|    15|\n",
      "|    16|\n",
      "|    17|\n",
      "|    18|\n",
      "|    19|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "|    10|\n",
      "|    11|\n",
      "|    12|\n",
      "|    13|\n",
      "|    14|\n",
      "|    15|\n",
      "|    16|\n",
      "|    17|\n",
      "|    18|\n",
      "|    19|\n",
      "|    20|\n",
      "|    21|\n",
      "|    22|\n",
      "|    23|\n",
      "|    24|\n",
      "|    25|\n",
      "|    26|\n",
      "|    27|\n",
      "|    28|\n",
      "|    29|\n",
      "|    30|\n",
      "|    31|\n",
      "|    32|\n",
      "|    33|\n",
      "|    34|\n",
      "|    35|\n",
      "|    36|\n",
      "|    37|\n",
      "|    38|\n",
      "|    39|\n",
      "|    40|\n",
      "|    41|\n",
      "|    42|\n",
      "|    43|\n",
      "|    44|\n",
      "|    45|\n",
      "|    46|\n",
      "|    47|\n",
      "|    48|\n",
      "|    49|\n",
      "+------+\n",
      "only showing top 50 rows\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "55ac6cc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:19:18.011266700Z",
     "start_time": "2026-02-13T21:19:05.978414300Z"
    }
   },
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Python list containing two rows\n",
    "emp = [Row(\"Jack\", 24), Row(\"Bobby\", 26)]\n",
    "\n",
    "# Convert Python data into Spark DataFrame\n",
    "emp_df = spark.createDataFrame(emp, [\"name\",\"age\"])\n",
    "\n",
    "emp_df.show()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "| Jack| 24|\n",
      "|Bobby| 26|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "id": "7750b44d",
   "metadata": {},
   "source": [
    "## DataFrame: create from file"
   ]
  },
  {
   "cell_type": "code",
   "id": "9b4cf528",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:19:18.410695400Z",
     "start_time": "2026-02-13T21:19:18.113530500Z"
    }
   },
   "source": [
    "# Create DataFrame from a file\n",
    "df = ( \n",
    "    spark.read\n",
    "    .option(\"header\", True)         # Tells Spark the first line is a header\n",
    "    .option(\"inferSchema\", True)    # Spark scans the column and infers data type; id will be an integer, country and capital will be a string\n",
    "    .format(\"csv\")                  \n",
    "    .load(\"sample_data1.csv\")\n",
    ")\n",
    "\n",
    "df.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------+\n",
      "| ID|       Country|    Capital|\n",
      "+---+--------------+-----------+\n",
      "|  1|        Canada|     Ottawa|\n",
      "|  2|        Mexico|Mexico City|\n",
      "|  3|        Brazil|   Brasilia|\n",
      "|  4|United Kingdom|     London|\n",
      "|  5|        France|      Paris|\n",
      "|  6|       Germany|     Berlin|\n",
      "|  7|         India|  New Delhi|\n",
      "|  8|         China|    Beijing|\n",
      "|  9|         Japan|      Tokyo|\n",
      "| 10|     Australia|   Canberra|\n",
      "+---+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "id": "a2e8fe5a",
   "metadata": {},
   "source": [
    "## DataFrame: Datasource\n",
    "\n",
    "Many different file types are possible, including CSV, JSON, ORC, Parquet, Text, Table, JDBC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef0ceb4",
   "metadata": {},
   "source": [
    "# Two Major Operations\n",
    "\n",
    "- All abstractions such as RDD and DataFrames offer two types of operation\n",
    "    - **Transformation:** construct a new RDD/DataFrame from a previous one\n",
    "    - **Action:** compute the result based on an RDD/DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93597aad",
   "metadata": {},
   "source": [
    "# Transformations\n",
    "\n",
    "## Transformations: `printSchema()` and `describe()`"
   ]
  },
  {
   "cell_type": "code",
   "id": "64d12110",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:19:18.554676100Z",
     "start_time": "2026-02-13T21:19:18.516681Z"
    }
   },
   "source": [
    "# Print the structure (data type of the columns) of the DataFrame\n",
    "df.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Capital: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "ee06ab7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:19:18.657976Z",
     "start_time": "2026-02-13T21:19:18.558679500Z"
    }
   },
   "source": [
    "# Describes the schema of the DataFrame\n",
    "df.describe()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, ID: string, Country: string, Capital: string]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "e4517cce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:19:18.726684900Z",
     "start_time": "2026-02-13T21:19:18.661974Z"
    }
   },
   "source": [
    "# Describe the structure of specific column\n",
    "df.select(\"Country\").describe()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, Country: string]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "id": "e3dc155b",
   "metadata": {},
   "source": [
    "## Transformations: `where()` and `filter()`"
   ]
  },
  {
   "cell_type": "code",
   "id": "20c958ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:19:19.123427300Z",
     "start_time": "2026-02-13T21:19:18.728672400Z"
    }
   },
   "source": [
    "df_population = ( \n",
    "                    spark.read\n",
    "                    .option(\"header\", True)         \n",
    "                    .option(\"inferSchema\", True) \n",
    "                    .format(\"csv\")                  \n",
    "                    .load(\"sample_data2.csv\")\n",
    ")\n",
    "\n",
    "df_population.show()\n",
    "\n",
    "hundredK_plus = df_population.filter(\"Population >= 100000\")\n",
    "hundredK_plus.show()\n",
    "\n",
    "under_50K = df_population.where(\"Population <= 50000\")\n",
    "under_50K.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+----------+\n",
      "| ID|               Town|Population|\n",
      "+---+-------------------+----------+\n",
      "|  1|             London|   8982000|\n",
      "|  2|         Birmingham|   1141000|\n",
      "|  3|         Manchester|    553230|\n",
      "|  4|              Leeds|    789200|\n",
      "|  5|            Glasgow|    635640|\n",
      "|  6|Newcastle upon Tyne|    300820|\n",
      "|  7|            Bristol|    463400|\n",
      "|  8|          Sheffield|    584853|\n",
      "|  9|          Liverpool|    498042|\n",
      "| 10|          Cambridge|    125000|\n",
      "| 11|             Oxford|    154000|\n",
      "| 12|               Bath|     89000|\n",
      "| 13|               York|    209200|\n",
      "| 14|         Canterbury|     55000|\n",
      "| 15|          Lichfield|     34000|\n",
      "| 16|         St Andrews|     17000|\n",
      "| 17|          Inverness|     47000|\n",
      "| 18|           Stirling|     37000|\n",
      "| 19|           Aberdeen|    198000|\n",
      "| 20|             Dundee|    148000|\n",
      "+---+-------------------+----------+\n",
      "\n",
      "+---+-------------------+----------+\n",
      "| ID|               Town|Population|\n",
      "+---+-------------------+----------+\n",
      "|  1|             London|   8982000|\n",
      "|  2|         Birmingham|   1141000|\n",
      "|  3|         Manchester|    553230|\n",
      "|  4|              Leeds|    789200|\n",
      "|  5|            Glasgow|    635640|\n",
      "|  6|Newcastle upon Tyne|    300820|\n",
      "|  7|            Bristol|    463400|\n",
      "|  8|          Sheffield|    584853|\n",
      "|  9|          Liverpool|    498042|\n",
      "| 10|          Cambridge|    125000|\n",
      "| 11|             Oxford|    154000|\n",
      "| 13|               York|    209200|\n",
      "| 19|           Aberdeen|    198000|\n",
      "| 20|             Dundee|    148000|\n",
      "+---+-------------------+----------+\n",
      "\n",
      "+---+----------+----------+\n",
      "| ID|      Town|Population|\n",
      "+---+----------+----------+\n",
      "| 15| Lichfield|     34000|\n",
      "| 16|St Andrews|     17000|\n",
      "| 17| Inverness|     47000|\n",
      "| 18|  Stirling|     37000|\n",
      "+---+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "id": "ce951e13",
   "metadata": {},
   "source": [
    "## Transformation: `distinct()` and\t`limit()` "
   ]
  },
  {
   "cell_type": "code",
   "id": "e95a04f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:19:19.687593900Z",
     "start_time": "2026-02-13T21:19:19.126472300Z"
    }
   },
   "source": [
    "df_town_village = ( \n",
    "                    spark.read\n",
    "                    .option(\"header\", True)         \n",
    "                    .option(\"inferSchema\", True) \n",
    "                    .format(\"csv\")                  \n",
    "                    .load(\"sample_data3.csv\")\n",
    ")\n",
    "\n",
    "df_town_village.show()\n",
    "\n",
    "unique_county = df_town_village.select(\"County\").distinct()\n",
    "unique_county.show()\n",
    "\n",
    "N = 5\n",
    "shortN_list = df_town_village.limit(N)\n",
    "shortN_list.show()\n",
    "\n",
    "# Alternate usage\n",
    "df_town_village.limit(N).show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|              County|  Town/Village|\n",
      "+--------------------+--------------+\n",
      "|       Aberdeenshire|      Aberdeen|\n",
      "|       Aberdeenshire|         Banff|\n",
      "|       Aberdeenshire|     Peterhead|\n",
      "|       Aberdeenshire|      Banchory|\n",
      "|                Fife|    St Andrews|\n",
      "|                Fife|         Cupar|\n",
      "|                Fife|    Anstruther|\n",
      "|                Fife|     Kirkcaldy|\n",
      "|            Highland|     Inverness|\n",
      "|            Highland|  Fort William|\n",
      "|            Highland|     Kingussie|\n",
      "|            Highland|        Thurso|\n",
      "|Dumfries and Gall...|      Dumfries|\n",
      "|Dumfries and Gall...|Castle Douglas|\n",
      "|Dumfries and Gall...|     Stranraer|\n",
      "|   Perth and Kinross|         Perth|\n",
      "|   Perth and Kinross|        Crieff|\n",
      "|   Perth and Kinross|   Blairgowrie|\n",
      "|            Stirling|      Stirling|\n",
      "|            Stirling|     Callander|\n",
      "+--------------------+--------------+\n",
      "\n",
      "+--------------------+\n",
      "|              County|\n",
      "+--------------------+\n",
      "|            Highland|\n",
      "|Dumfries and Gall...|\n",
      "|   Perth and Kinross|\n",
      "|                Fife|\n",
      "|       Aberdeenshire|\n",
      "|            Stirling|\n",
      "+--------------------+\n",
      "\n",
      "+-------------+------------+\n",
      "|       County|Town/Village|\n",
      "+-------------+------------+\n",
      "|Aberdeenshire|    Aberdeen|\n",
      "|Aberdeenshire|       Banff|\n",
      "|Aberdeenshire|   Peterhead|\n",
      "|Aberdeenshire|    Banchory|\n",
      "|         Fife|  St Andrews|\n",
      "+-------------+------------+\n",
      "\n",
      "+-------------+------------+\n",
      "|       County|Town/Village|\n",
      "+-------------+------------+\n",
      "|Aberdeenshire|    Aberdeen|\n",
      "|Aberdeenshire|       Banff|\n",
      "|Aberdeenshire|   Peterhead|\n",
      "|Aberdeenshire|    Banchory|\n",
      "|         Fife|  St Andrews|\n",
      "+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "id": "89a3dc99",
   "metadata": {},
   "source": [
    "## Transformation: Sorting using `sort()` or `orderBy()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992b324c",
   "metadata": {},
   "source": [
    "### Basic sorting"
   ]
  },
  {
   "cell_type": "code",
   "id": "5ab48fd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:19:20.073425600Z",
     "start_time": "2026-02-13T21:19:19.727673900Z"
    }
   },
   "source": [
    "# Sort by a single column\n",
    "sorted = df_town_village.sort(\"County\")\n",
    "sorted.show()\n",
    "\n",
    "# Sort by multiple columns\n",
    "sorted = df_town_village.sort(\"County\", \"Town/Village\")\n",
    "sorted.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|              County|  Town/Village|\n",
      "+--------------------+--------------+\n",
      "|       Aberdeenshire|      Aberdeen|\n",
      "|       Aberdeenshire|         Banff|\n",
      "|       Aberdeenshire|     Peterhead|\n",
      "|       Aberdeenshire|      Banchory|\n",
      "|Dumfries and Gall...|      Dumfries|\n",
      "|Dumfries and Gall...|Castle Douglas|\n",
      "|Dumfries and Gall...|     Stranraer|\n",
      "|                Fife|    St Andrews|\n",
      "|                Fife|         Cupar|\n",
      "|                Fife|    Anstruther|\n",
      "|                Fife|     Kirkcaldy|\n",
      "|            Highland|     Inverness|\n",
      "|            Highland|  Fort William|\n",
      "|            Highland|     Kingussie|\n",
      "|            Highland|        Thurso|\n",
      "|   Perth and Kinross|         Perth|\n",
      "|   Perth and Kinross|        Crieff|\n",
      "|   Perth and Kinross|   Blairgowrie|\n",
      "|            Stirling|      Stirling|\n",
      "|            Stirling|     Callander|\n",
      "+--------------------+--------------+\n",
      "\n",
      "+--------------------+--------------+\n",
      "|              County|  Town/Village|\n",
      "+--------------------+--------------+\n",
      "|       Aberdeenshire|      Aberdeen|\n",
      "|       Aberdeenshire|      Banchory|\n",
      "|       Aberdeenshire|         Banff|\n",
      "|       Aberdeenshire|     Peterhead|\n",
      "|Dumfries and Gall...|Castle Douglas|\n",
      "|Dumfries and Gall...|      Dumfries|\n",
      "|Dumfries and Gall...|     Stranraer|\n",
      "|                Fife|    Anstruther|\n",
      "|                Fife|         Cupar|\n",
      "|                Fife|     Kirkcaldy|\n",
      "|                Fife|    St Andrews|\n",
      "|            Highland|  Fort William|\n",
      "|            Highland|     Inverness|\n",
      "|            Highland|     Kingussie|\n",
      "|            Highland|        Thurso|\n",
      "|   Perth and Kinross|   Blairgowrie|\n",
      "|   Perth and Kinross|        Crieff|\n",
      "|   Perth and Kinross|         Perth|\n",
      "|            Stirling|     Callander|\n",
      "|            Stirling|      Stirling|\n",
      "+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "id": "815e9fe0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:19:20.301404900Z",
     "start_time": "2026-02-13T21:19:20.113422600Z"
    }
   },
   "source": [
    "# Order by a single column\n",
    "sorted = df_town_village.orderBy(\"Town/Village\")\n",
    "sorted.show()\n",
    "\n",
    "# Order by multiple columns\n",
    "sorted = df_town_village.orderBy(\"County\", \"Town/Village\")\n",
    "sorted.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|              County|  Town/Village|\n",
      "+--------------------+--------------+\n",
      "|       Aberdeenshire|      Aberdeen|\n",
      "|                Fife|    Anstruther|\n",
      "|       Aberdeenshire|      Banchory|\n",
      "|       Aberdeenshire|         Banff|\n",
      "|   Perth and Kinross|   Blairgowrie|\n",
      "|            Stirling|     Callander|\n",
      "|Dumfries and Gall...|Castle Douglas|\n",
      "|   Perth and Kinross|        Crieff|\n",
      "|                Fife|         Cupar|\n",
      "|Dumfries and Gall...|      Dumfries|\n",
      "|            Highland|  Fort William|\n",
      "|            Highland|     Inverness|\n",
      "|            Highland|     Kingussie|\n",
      "|                Fife|     Kirkcaldy|\n",
      "|   Perth and Kinross|         Perth|\n",
      "|       Aberdeenshire|     Peterhead|\n",
      "|                Fife|    St Andrews|\n",
      "|            Stirling|      Stirling|\n",
      "|Dumfries and Gall...|     Stranraer|\n",
      "|            Highland|        Thurso|\n",
      "+--------------------+--------------+\n",
      "\n",
      "+--------------------+--------------+\n",
      "|              County|  Town/Village|\n",
      "+--------------------+--------------+\n",
      "|       Aberdeenshire|      Aberdeen|\n",
      "|       Aberdeenshire|      Banchory|\n",
      "|       Aberdeenshire|         Banff|\n",
      "|       Aberdeenshire|     Peterhead|\n",
      "|Dumfries and Gall...|Castle Douglas|\n",
      "|Dumfries and Gall...|      Dumfries|\n",
      "|Dumfries and Gall...|     Stranraer|\n",
      "|                Fife|    Anstruther|\n",
      "|                Fife|         Cupar|\n",
      "|                Fife|     Kirkcaldy|\n",
      "|                Fife|    St Andrews|\n",
      "|            Highland|  Fort William|\n",
      "|            Highland|     Inverness|\n",
      "|            Highland|     Kingussie|\n",
      "|            Highland|        Thurso|\n",
      "|   Perth and Kinross|   Blairgowrie|\n",
      "|   Perth and Kinross|        Crieff|\n",
      "|   Perth and Kinross|         Perth|\n",
      "|            Stirling|     Callander|\n",
      "|            Stirling|      Stirling|\n",
      "+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "id": "5e35e3a3",
   "metadata": {},
   "source": [
    "### Specifying sort direction"
   ]
  },
  {
   "cell_type": "code",
   "id": "fafa0363",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:19:20.615474Z",
     "start_time": "2026-02-13T21:19:20.412403600Z"
    }
   },
   "source": [
    "from pyspark.sql.functions import desc, asc \n",
    "\n",
    "sorted = df_town_village.orderBy(desc(\"Town/Village\"))\n",
    "sorted.show()\n",
    "\n",
    "sorted = df_town_village.orderBy(asc(\"County\"), desc(\"Town/Village\"))\n",
    "sorted.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|              County|  Town/Village|\n",
      "+--------------------+--------------+\n",
      "|            Highland|        Thurso|\n",
      "|Dumfries and Gall...|     Stranraer|\n",
      "|            Stirling|      Stirling|\n",
      "|                Fife|    St Andrews|\n",
      "|       Aberdeenshire|     Peterhead|\n",
      "|   Perth and Kinross|         Perth|\n",
      "|                Fife|     Kirkcaldy|\n",
      "|            Highland|     Kingussie|\n",
      "|            Highland|     Inverness|\n",
      "|            Highland|  Fort William|\n",
      "|Dumfries and Gall...|      Dumfries|\n",
      "|                Fife|         Cupar|\n",
      "|   Perth and Kinross|        Crieff|\n",
      "|Dumfries and Gall...|Castle Douglas|\n",
      "|            Stirling|     Callander|\n",
      "|   Perth and Kinross|   Blairgowrie|\n",
      "|       Aberdeenshire|         Banff|\n",
      "|       Aberdeenshire|      Banchory|\n",
      "|                Fife|    Anstruther|\n",
      "|       Aberdeenshire|      Aberdeen|\n",
      "+--------------------+--------------+\n",
      "\n",
      "+--------------------+--------------+\n",
      "|              County|  Town/Village|\n",
      "+--------------------+--------------+\n",
      "|       Aberdeenshire|     Peterhead|\n",
      "|       Aberdeenshire|         Banff|\n",
      "|       Aberdeenshire|      Banchory|\n",
      "|       Aberdeenshire|      Aberdeen|\n",
      "|Dumfries and Gall...|     Stranraer|\n",
      "|Dumfries and Gall...|      Dumfries|\n",
      "|Dumfries and Gall...|Castle Douglas|\n",
      "|                Fife|    St Andrews|\n",
      "|                Fife|     Kirkcaldy|\n",
      "|                Fife|         Cupar|\n",
      "|                Fife|    Anstruther|\n",
      "|            Highland|        Thurso|\n",
      "|            Highland|     Kingussie|\n",
      "|            Highland|     Inverness|\n",
      "|            Highland|  Fort William|\n",
      "|   Perth and Kinross|         Perth|\n",
      "|   Perth and Kinross|        Crieff|\n",
      "|   Perth and Kinross|   Blairgowrie|\n",
      "|            Stirling|      Stirling|\n",
      "|            Stirling|     Callander|\n",
      "+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "id": "170dd105",
   "metadata": {},
   "source": [
    "## Transformation: Sampling data using `sample`"
   ]
  },
  {
   "cell_type": "code",
   "id": "d3bef7a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:19:20.965028900Z",
     "start_time": "2026-02-13T21:19:20.763920900Z"
    }
   },
   "source": [
    "with_replacement = False    # Sample without replacement; each row can appear at most once \n",
    "fraction = 0.50             # Roughly 50% of the rows are selected\n",
    "seed = None                 # Sets the random seed for reproducibility; if an integer sample value is set it produces the same sample everytime \n",
    "\n",
    "sample = df_town_village.sample(with_replacement, fraction, seed)\n",
    "sample.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|              County|  Town/Village|\n",
      "+--------------------+--------------+\n",
      "|       Aberdeenshire|     Peterhead|\n",
      "|                Fife|    Anstruther|\n",
      "|                Fife|     Kirkcaldy|\n",
      "|            Highland|     Inverness|\n",
      "|Dumfries and Gall...|Castle Douglas|\n",
      "|   Perth and Kinross|         Perth|\n",
      "|            Stirling|      Stirling|\n",
      "|            Stirling|     Callander|\n",
      "+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "id": "738e0474",
   "metadata": {},
   "source": [
    "## Transformation: Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "id": "e4d49bf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:19:21.518190400Z",
     "start_time": "2026-02-13T21:19:21.075504Z"
    }
   },
   "source": [
    "from pyspark.sql.functions import count, countDistinct \n",
    "\n",
    "df_town_village.select(count(\"County\")).show()\n",
    "\n",
    "df_town_village.select(countDistinct(\"County\")).show()\n",
    "\n",
    "# min, max, avg, first, last and groupBy functions are available and self explanatory\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|count(County)|\n",
      "+-------------+\n",
      "|           20|\n",
      "+-------------+\n",
      "\n",
      "+----------------------+\n",
      "|count(DISTINCT County)|\n",
      "+----------------------+\n",
      "|                     6|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "id": "7114d6da",
   "metadata": {},
   "source": [
    "## DataFrame: Some Actions"
   ]
  },
  {
   "cell_type": "code",
   "id": "3458c7e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:19:21.915427400Z",
     "start_time": "2026-02-13T21:19:21.567951800Z"
    }
   },
   "source": [
    "# first()\n",
    "row = df_town_village.first()\n",
    "print(row)\n",
    "print(row[\"Town/Village\"])      #Access column of the first row\n",
    "\n",
    "# show()\n",
    "df_town_village.show()\n",
    "N = 6\n",
    "df_town_village.show(N)\n",
    "\n",
    "# take(N)\n",
    "N = 4\n",
    "rows = df_town_village.take(N)  #Similar to first, but returns multiple rows\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# collect()\n",
    "all_rows = df_town_village.collect()    #Returns all rows as a list of objects\n",
    "# Note: if the DataFrame is large, then may not work as all memory is brought into memory\n",
    "# Use this for small datasets or debugging\n",
    "for row in all_rows:\n",
    "    print(row)\n",
    "\n",
    "#count()\n",
    "print(f\"Total rows: {df_town_village.count()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(County='Aberdeenshire', Town/Village='Aberdeen')\n",
      "Aberdeen\n",
      "+--------------------+--------------+\n",
      "|              County|  Town/Village|\n",
      "+--------------------+--------------+\n",
      "|       Aberdeenshire|      Aberdeen|\n",
      "|       Aberdeenshire|         Banff|\n",
      "|       Aberdeenshire|     Peterhead|\n",
      "|       Aberdeenshire|      Banchory|\n",
      "|                Fife|    St Andrews|\n",
      "|                Fife|         Cupar|\n",
      "|                Fife|    Anstruther|\n",
      "|                Fife|     Kirkcaldy|\n",
      "|            Highland|     Inverness|\n",
      "|            Highland|  Fort William|\n",
      "|            Highland|     Kingussie|\n",
      "|            Highland|        Thurso|\n",
      "|Dumfries and Gall...|      Dumfries|\n",
      "|Dumfries and Gall...|Castle Douglas|\n",
      "|Dumfries and Gall...|     Stranraer|\n",
      "|   Perth and Kinross|         Perth|\n",
      "|   Perth and Kinross|        Crieff|\n",
      "|   Perth and Kinross|   Blairgowrie|\n",
      "|            Stirling|      Stirling|\n",
      "|            Stirling|     Callander|\n",
      "+--------------------+--------------+\n",
      "\n",
      "+-------------+------------+\n",
      "|       County|Town/Village|\n",
      "+-------------+------------+\n",
      "|Aberdeenshire|    Aberdeen|\n",
      "|Aberdeenshire|       Banff|\n",
      "|Aberdeenshire|   Peterhead|\n",
      "|Aberdeenshire|    Banchory|\n",
      "|         Fife|  St Andrews|\n",
      "|         Fife|       Cupar|\n",
      "+-------------+------------+\n",
      "only showing top 6 rows\n",
      "Row(County='Aberdeenshire', Town/Village='Aberdeen')\n",
      "Row(County='Aberdeenshire', Town/Village='Banff')\n",
      "Row(County='Aberdeenshire', Town/Village='Peterhead')\n",
      "Row(County='Aberdeenshire', Town/Village='Banchory')\n",
      "Row(County='Aberdeenshire', Town/Village='Aberdeen')\n",
      "Row(County='Aberdeenshire', Town/Village='Banff')\n",
      "Row(County='Aberdeenshire', Town/Village='Peterhead')\n",
      "Row(County='Aberdeenshire', Town/Village='Banchory')\n",
      "Row(County='Fife', Town/Village='St Andrews')\n",
      "Row(County='Fife', Town/Village='Cupar')\n",
      "Row(County='Fife', Town/Village='Anstruther')\n",
      "Row(County='Fife', Town/Village='Kirkcaldy')\n",
      "Row(County='Highland', Town/Village='Inverness')\n",
      "Row(County='Highland', Town/Village='Fort William')\n",
      "Row(County='Highland', Town/Village='Kingussie')\n",
      "Row(County='Highland', Town/Village='Thurso')\n",
      "Row(County='Dumfries and Galloway', Town/Village='Dumfries')\n",
      "Row(County='Dumfries and Galloway', Town/Village='Castle Douglas')\n",
      "Row(County='Dumfries and Galloway', Town/Village='Stranraer')\n",
      "Row(County='Perth and Kinross', Town/Village='Perth')\n",
      "Row(County='Perth and Kinross', Town/Village='Crieff')\n",
      "Row(County='Perth and Kinross', Town/Village='Blairgowrie')\n",
      "Row(County='Stirling', Town/Village='Stirling')\n",
      "Row(County='Stirling', Town/Village='Callander')\n",
      "Total rows: 20\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "id": "8861f3f1",
   "metadata": {},
   "source": [
    "# RDD\n",
    "\n",
    "- Low level but still relevant in some cases:\n",
    "- Raw data processing e.g. text file without structure.\n",
    "    - Creating new RDDs\n",
    "    - Transforming existing RDDs\n",
    "    - Computing results from RDDs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fe500b",
   "metadata": {},
   "source": [
    "## Create RDD"
   ]
  },
  {
   "cell_type": "code",
   "id": "846fcfe1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:19:22.040275300Z",
     "start_time": "2026-02-13T21:19:21.933986900Z"
    }
   },
   "source": [
    "# From an existing file\n",
    "lines = spark.sparkContext.textFile(\"README_dummy.md\")\n",
    "\n",
    "# or\n",
    "sc = spark.sparkContext\n",
    "lines = sc.textFile(\"README_dummy.md\")\n",
    "\n",
    "# Collect all lines into a Python list\n",
    "all_lines = lines.collect()\n",
    "\n",
    "# Print each line\n",
    "for line in all_lines:\n",
    "    print(line)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Spark Sample Project\n",
      "**Note:** this file will be used in some of the examples shown in `spark-test.ipynb`\n",
      "\n",
      "This is a **dummy project** to demonstrate working with **Apache Spark** using Python (`pyspark`).  \n",
      "It contains some sample data, Spark DataFrame examples, and basic transformations.\n",
      "\n",
      "---\n",
      "\n",
      "## Overview\n",
      "\n",
      "Apache Spark is a **distributed computing framework** that allows processing of large datasets across multiple machines.  \n",
      "It provides APIs for Python, Scala, Java, and R, with built-in support for SQL, machine learning, and streaming.\n",
      "\n",
      "In this project, we will use **Spark DataFrames** to handle tabular data efficiently.\n",
      "\n",
      "---\n",
      "\n",
      "## Sample Spark Operations\n",
      "\n",
      "```python\n",
      "from pyspark.sql import SparkSession\n",
      "from pyspark.sql.functions import col\n",
      "\n",
      "# Create Spark session\n",
      "spark = SparkSession.builder.appName(\"DummyProject\").getOrCreate()\n",
      "\n",
      "# Load sample CSV\n",
      "df = spark.read.option(\"header\", True).csv(\"sample_data.csv\")\n",
      "\n",
      "# Show first 5 rows\n",
      "df.show(5)\n",
      "\n",
      "# Filter rows using Spark\n",
      "high_population = df.filter(col(\"population\") > 100000)\n",
      "high_population.show()\n",
      "\n",
      "# Count total rows\n",
      "total_rows = df.count()\n",
      "print(f\"Total rows in DataFrame: {total_rows}\")\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "id": "8a299b4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:19:22.131160400Z",
     "start_time": "2026-02-13T21:19:22.042274300Z"
    }
   },
   "source": [
    "# From a list\n",
    "numbers = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "# numbers is an RDD containing numbers 1 to 8\n",
    "# The data is split into partitions and can be processed in parallel\n",
    "\n",
    "# Aggregate the partitions and print all numbers\n",
    "print(numbers.collect())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "id": "94f78071",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:19:24.703951600Z",
     "start_time": "2026-02-13T21:19:22.132159800Z"
    }
   },
   "source": [
    "# Create an RDD from a file \n",
    "lines = sc.textFile(\"README_dummy.md\")\n",
    "\n",
    "# Create new RDD with lines containing Spark\n",
    "lines = lines.filter(lambda x: 'Spark' in x)\n",
    "\n",
    "# Count the number of items in this RDD\n",
    "# Note: The above two lines doesn't do anything \n",
    "# The statement below will read the file and do the computation\n",
    "print(lines.count())\n",
    "\n",
    "# The statement below will read the file and do the computation again\n",
    "print(lines.count())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "id": "200690c4",
   "metadata": {},
   "source": [
    "## RDD - Persisting\n",
    "\n",
    "- Spark recomputes RDDs each time an action is performed on it\n",
    "    - By default RDD is not stored in memory\n",
    "    - The `persist()` function stores an RDD permanently"
   ]
  },
  {
   "cell_type": "code",
   "id": "d479898f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:19:27.324189200Z",
     "start_time": "2026-02-13T21:19:24.775296900Z"
    }
   },
   "source": [
    "lines = sc.textFile(\"README_dummy.md\")\n",
    "lines = lines.filter(lambda x: 'Spark' in x)\n",
    "\n",
    "# Load and store dataset in memory\n",
    "lines.persist()\n",
    "\n",
    "# Perform computation on the stored dataset\n",
    "print(lines.count())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "id": "b0cb9faf",
   "metadata": {},
   "source": [
    "## Basic RDD Transformation Functions\n",
    "\n",
    "- Construct an RDD from a previous one\n",
    "    - Performed on one or more RDDs\n",
    "    - Return a new RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74bfbfc",
   "metadata": {},
   "source": [
    "### `filter()`\n",
    "- Takes in a function, returns an RDD that only has elements that pass the filter() function"
   ]
  },
  {
   "cell_type": "code",
   "id": "a1a6e708",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:19:28.695402400Z",
     "start_time": "2026-02-13T21:19:27.356133500Z"
    }
   },
   "source": [
    "lines = sc.textFile(\"README_dummy.md\")\n",
    "\n",
    "# Create a new RDD consisting lines that contain ‘Spark’\n",
    "lines = lines.filter(lambda x: 'Spark' in x)\n",
    "\n",
    "all_lines = lines.collect()\n",
    "\n",
    "for line in all_lines:\n",
    "    print(line)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Spark Sample Project\n",
      "This is a **dummy project** to demonstrate working with **Apache Spark** using Python (`pyspark`).  \n",
      "It contains some sample data, Spark DataFrame examples, and basic transformations.\n",
      "Apache Spark is a **distributed computing framework** that allows processing of large datasets across multiple machines.  \n",
      "In this project, we will use **Spark DataFrames** to handle tabular data efficiently.\n",
      "## Sample Spark Operations\n",
      "from pyspark.sql import SparkSession\n",
      "# Create Spark session\n",
      "spark = SparkSession.builder.appName(\"DummyProject\").getOrCreate()\n",
      "# Filter rows using Spark\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "id": "e7c319b1",
   "metadata": {},
   "source": [
    "### `map()`\n",
    "- Takes in a function and applies it to each element in the RDD"
   ]
  },
  {
   "cell_type": "code",
   "id": "3b415d0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:19:30.075118400Z",
     "start_time": "2026-02-13T21:19:28.719954500Z"
    }
   },
   "source": [
    "lines = sc.textFile(\"README_dummy.md\")\n",
    "\n",
    "# Create a new RDD in which all strings are in uppercase\n",
    "lines = lines.map(lambda x: x.upper())\n",
    "\n",
    "all_lines = lines.collect()\n",
    "\n",
    "for line in all_lines:\n",
    "    print(line)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# SPARK SAMPLE PROJECT\n",
      "**NOTE:** THIS FILE WILL BE USED IN SOME OF THE EXAMPLES SHOWN IN `SPARK-TEST.IPYNB`\n",
      "\n",
      "THIS IS A **DUMMY PROJECT** TO DEMONSTRATE WORKING WITH **APACHE SPARK** USING PYTHON (`PYSPARK`).  \n",
      "IT CONTAINS SOME SAMPLE DATA, SPARK DATAFRAME EXAMPLES, AND BASIC TRANSFORMATIONS.\n",
      "\n",
      "---\n",
      "\n",
      "## OVERVIEW\n",
      "\n",
      "APACHE SPARK IS A **DISTRIBUTED COMPUTING FRAMEWORK** THAT ALLOWS PROCESSING OF LARGE DATASETS ACROSS MULTIPLE MACHINES.  \n",
      "IT PROVIDES APIS FOR PYTHON, SCALA, JAVA, AND R, WITH BUILT-IN SUPPORT FOR SQL, MACHINE LEARNING, AND STREAMING.\n",
      "\n",
      "IN THIS PROJECT, WE WILL USE **SPARK DATAFRAMES** TO HANDLE TABULAR DATA EFFICIENTLY.\n",
      "\n",
      "---\n",
      "\n",
      "## SAMPLE SPARK OPERATIONS\n",
      "\n",
      "```PYTHON\n",
      "FROM PYSPARK.SQL IMPORT SPARKSESSION\n",
      "FROM PYSPARK.SQL.FUNCTIONS IMPORT COL\n",
      "\n",
      "# CREATE SPARK SESSION\n",
      "SPARK = SPARKSESSION.BUILDER.APPNAME(\"DUMMYPROJECT\").GETORCREATE()\n",
      "\n",
      "# LOAD SAMPLE CSV\n",
      "DF = SPARK.READ.OPTION(\"HEADER\", TRUE).CSV(\"SAMPLE_DATA.CSV\")\n",
      "\n",
      "# SHOW FIRST 5 ROWS\n",
      "DF.SHOW(5)\n",
      "\n",
      "# FILTER ROWS USING SPARK\n",
      "HIGH_POPULATION = DF.FILTER(COL(\"POPULATION\") > 100000)\n",
      "HIGH_POPULATION.SHOW()\n",
      "\n",
      "# COUNT TOTAL ROWS\n",
      "TOTAL_ROWS = DF.COUNT()\n",
      "PRINT(F\"TOTAL ROWS IN DATAFRAME: {TOTAL_ROWS}\")\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "id": "99285319",
   "metadata": {},
   "source": [
    "### `flatmap()`\n",
    "- Applies a function to each element in an RDD\n",
    "- Returns a sequence (list of elements)\n",
    "- The final RDD is flattened"
   ]
  },
  {
   "cell_type": "code",
   "id": "d5cbc518",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:19:41.567382500Z",
     "start_time": "2026-02-13T21:19:30.107455700Z"
    }
   },
   "source": [
    "lines = sc.parallelize([\n",
    "    \"I love Spark\",\n",
    "    \"Spark is awesome\",\n",
    "    \"Big data rocks\"\n",
    "])\n",
    "\n",
    "words_using_map = lines.map(lambda line: line.split(\" \"))\n",
    "print(words_using_map.collect())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'love', 'Spark'], ['Spark', 'is', 'awesome'], ['Big', 'data', 'rocks']]\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "id": "26194aad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:19:53.055205300Z",
     "start_time": "2026-02-13T21:19:41.581249Z"
    }
   },
   "source": [
    "words_using_flatmap = lines.flatMap(lambda line: line.split(\" \"))\n",
    "print(words_using_flatmap.collect())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'Spark', 'Spark', 'is', 'awesome', 'Big', 'data', 'rocks']\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "id": "f03683f6",
   "metadata": {},
   "source": [
    "### `distinct()`\n",
    "- Returns a new RDD with only distinct items"
   ]
  },
  {
   "cell_type": "code",
   "id": "21054f0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:20:16.579261100Z",
     "start_time": "2026-02-13T21:19:53.067727300Z"
    }
   },
   "source": [
    "numbers = sc.parallelize([0, 1, 2, 4, 7, 5, 4, 3, 2, 1, 1, 0])\n",
    "numbers = numbers.distinct()\n",
    "print(numbers.collect())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 7]\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "id": "e9720738",
   "metadata": {},
   "source": [
    "### `union(other)`\n",
    "- Returns a new RDD consisting of items from both sources"
   ]
  },
  {
   "cell_type": "code",
   "id": "3196c6eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:20:16.724774100Z",
     "start_time": "2026-02-13T21:20:16.605420300Z"
    }
   },
   "source": [
    "numbers = sc.parallelize([0, 1, 2, 3, 4])\n",
    "characters = sc.parallelize(['A', 'B', 'C', 'D', 'E'])\n",
    "result = numbers.union(characters)\n",
    "print(result.collect())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 'A', 'B', 'C', 'D', 'E']\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "id": "e22e8ea3",
   "metadata": {},
   "source": [
    "### `intersection(other)`\n",
    "- Returns a new RDD consisting of only items from both sources and removes all duplicates"
   ]
  },
  {
   "cell_type": "code",
   "id": "4c6bc156",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:20:36.311761700Z",
     "start_time": "2026-02-13T21:20:16.725783300Z"
    }
   },
   "source": [
    "number_list1 = sc.parallelize([0, 1, 2, 4, 6, 7, 8])\n",
    "number_list2 = sc.parallelize([0, 1, 3, 4, 5, 7])\n",
    "result = number_list1.intersection(number_list2)\n",
    "print(result.collect())"
   ],
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 118.0 failed 1 times, most recent failure: Lost task 4.0 in stage 118.0 (TID 443) (Monster executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:678)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:663)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1034)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeNextElementToStream(PythonRDD.scala:335)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeNextInputToStream(PythonRunner.scala:995)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:933)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:848)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:381)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1022)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\nCaused by: java.io.IOException: An established connection was aborted by the software in your host machine\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:132)\r\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:76)\r\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:53)\r\n\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:532)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:944)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:848)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:381)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1022)\r\n\t... 31 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\r\n\tat scala.collection.immutable.List.foreach(List.scala:323)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1017)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2496)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2517)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2536)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2561)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:205)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor109.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:678)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:663)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1034)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeNextElementToStream(PythonRDD.scala:335)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeNextInputToStream(PythonRunner.scala:995)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:933)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:848)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:381)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1022)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.io.IOException: An established connection was aborted by the software in your host machine\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:132)\r\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:76)\r\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:53)\r\n\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:532)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:944)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:848)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:381)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1022)\r\n\t... 31 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mPy4JJavaError\u001B[39m                             Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[56]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      2\u001B[39m number_list2 = sc.parallelize([\u001B[32m0\u001B[39m, \u001B[32m1\u001B[39m, \u001B[32m3\u001B[39m, \u001B[32m4\u001B[39m, \u001B[32m5\u001B[39m, \u001B[32m7\u001B[39m])\n\u001B[32m      3\u001B[39m result = number_list1.intersection(number_list2)\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[43mresult\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\cs5052-practical1\\.venv\\Lib\\site-packages\\pyspark\\core\\rdd.py:1700\u001B[39m, in \u001B[36mRDD.collect\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1698\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m SCCallSiteSync(\u001B[38;5;28mself\u001B[39m.context):\n\u001B[32m   1699\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.ctx._jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1700\u001B[39m     sock_info = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mctx\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_jvm\u001B[49m\u001B[43m.\u001B[49m\u001B[43mPythonRDD\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcollectAndServe\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_jrdd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrdd\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1701\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(_load_from_socket(sock_info, \u001B[38;5;28mself\u001B[39m._jrdd_deserializer))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\cs5052-practical1\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001B[39m, in \u001B[36mJavaMember.__call__\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m   1356\u001B[39m command = proto.CALL_COMMAND_NAME +\\\n\u001B[32m   1357\u001B[39m     \u001B[38;5;28mself\u001B[39m.command_header +\\\n\u001B[32m   1358\u001B[39m     args_command +\\\n\u001B[32m   1359\u001B[39m     proto.END_COMMAND_PART\n\u001B[32m   1361\u001B[39m answer = \u001B[38;5;28mself\u001B[39m.gateway_client.send_command(command)\n\u001B[32m-> \u001B[39m\u001B[32m1362\u001B[39m return_value = \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1363\u001B[39m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1365\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[32m   1366\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[33m\"\u001B[39m\u001B[33m_detach\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\cs5052-practical1\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:263\u001B[39m, in \u001B[36mcapture_sql_exception.<locals>.deco\u001B[39m\u001B[34m(*a, **kw)\u001B[39m\n\u001B[32m    260\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpy4j\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mprotocol\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Py4JJavaError\n\u001B[32m    262\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m263\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    264\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    265\u001B[39m     converted = convert_exception(e.java_exception)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\cs5052-practical1\\.venv\\Lib\\site-packages\\py4j\\protocol.py:327\u001B[39m, in \u001B[36mget_return_value\u001B[39m\u001B[34m(answer, gateway_client, target_id, name)\u001B[39m\n\u001B[32m    325\u001B[39m value = OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[32m2\u001B[39m:], gateway_client)\n\u001B[32m    326\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[32m1\u001B[39m] == REFERENCE_TYPE:\n\u001B[32m--> \u001B[39m\u001B[32m327\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[32m    328\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m.\n\u001B[32m    329\u001B[39m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m, name), value)\n\u001B[32m    330\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    331\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[32m    332\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m.\n\u001B[32m    333\u001B[39m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m, name, value))\n",
      "\u001B[31mPy4JJavaError\u001B[39m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 118.0 failed 1 times, most recent failure: Lost task 4.0 in stage 118.0 (TID 443) (Monster executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:678)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:663)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1034)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeNextElementToStream(PythonRDD.scala:335)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeNextInputToStream(PythonRunner.scala:995)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:933)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:848)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:381)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1022)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\nCaused by: java.io.IOException: An established connection was aborted by the software in your host machine\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:132)\r\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:76)\r\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:53)\r\n\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:532)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:944)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:848)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:381)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1022)\r\n\t... 31 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\r\n\tat scala.collection.immutable.List.foreach(List.scala:323)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1017)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2496)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2517)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2536)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2561)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:205)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor109.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:678)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:663)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1034)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeNextElementToStream(PythonRDD.scala:335)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeNextInputToStream(PythonRunner.scala:995)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:933)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:848)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:381)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1022)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:263)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:265)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.io.IOException: An established connection was aborted by the software in your host machine\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:132)\r\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:76)\r\n\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:53)\r\n\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:532)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:944)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:848)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:381)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1022)\r\n\t... 31 more\r\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "id": "d14cf11e",
   "metadata": {},
   "source": [
    "### `subtract(other)`\n",
    "- Returns a new RDD consisting of only items in the first RDD but not in the other one"
   ]
  },
  {
   "cell_type": "code",
   "id": "4fb18594",
   "metadata": {},
   "source": [
    "number_list1 = sc.parallelize([0, 1, 2, 4, 6, 7, 8])\n",
    "number_list2 = sc.parallelize([0, 1, 3, 4, 5, 7])\n",
    "result = number_list1.subtract(number_list2)\n",
    "print(result.collect())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4f688102",
   "metadata": {},
   "source": [
    "## Basic RDD Action Functions\n",
    "\n",
    "- Compute result based on RDD(s)\n",
    "    - Performed on one or more RDD(s)\n",
    "    - Return a result, which is not an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aaf4e5",
   "metadata": {},
   "source": [
    "### `first()`\n",
    "- Returns the first item in an RDD"
   ]
  },
  {
   "cell_type": "code",
   "id": "9f8ac995",
   "metadata": {},
   "source": [
    "numbers = sc.parallelize([0, 1, 2, 3, 4, 5, 6, 7, 8]) \n",
    "print(numbers.first())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3b64e2b3",
   "metadata": {},
   "source": [
    "### `collect()`\n",
    "- Returns a list containing the entire RDD's content"
   ]
  },
  {
   "cell_type": "code",
   "id": "2be6d975",
   "metadata": {},
   "source": [
    "numbers = sc.parallelize([0, 1, 2, 3, 4, 5, 6, 7, 8]) \n",
    "print(numbers.collect())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "95c7cdb2",
   "metadata": {},
   "source": [
    "### `count()`\n",
    "- Returns the number of items in an RDD"
   ]
  },
  {
   "cell_type": "code",
   "id": "e86dcef1",
   "metadata": {},
   "source": [
    "numbers = sc.parallelize([0, 1, 2, 3, 4, 5, 6, 7, 8]) \n",
    "print(numbers.count())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "47196a9e",
   "metadata": {},
   "source": [
    "### `reduce(function)`\n",
    "- Takes a function that operates on two elements and returns a new element"
   ]
  },
  {
   "cell_type": "code",
   "id": "0c9548b4",
   "metadata": {},
   "source": [
    "numbers = sc.parallelize([1, 2, 3, 4, 5]) \n",
    "result = numbers.reduce(lambda x, y: x * y)\n",
    "print(result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "be774f67",
   "metadata": {},
   "source": [
    "### `takeOrdered(num, ordering)`\n",
    "- Return a number of items based on the provided ordering"
   ]
  },
  {
   "cell_type": "code",
   "id": "a08f1183",
   "metadata": {},
   "source": [
    "numbers = sc.parallelize([8, 0, 4, 6, 9, 7, 2, 1, 5, 3])\n",
    "\n",
    "# Return five smallest numbers from the list\n",
    "print(numbers.takeOrdered(5, lambda x: x))\n",
    "\n",
    "# Return five largest numbers from the list\n",
    "print(numbers.takeOrdered(5, lambda x: -x))\n",
    "# Note: How this function works:\n",
    "# Original numbers: 8, 0, 4, 6, 9, 7, 2, 1, 5, 3\n",
    "# Negated numbers: -8, 0, -4, -6, -9, -7, -2, -1, -5, -3\n",
    "# 5 smallest of these: -9, -8, -7, -6, -5\n",
    "# Negate back: 9, 8, 7, 6, 5"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
