{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "486b40ec",
   "metadata": {},
   "source": [
    "# CS5052 - Spark Programming\n",
    "> Created by: Professor Blesson Varghese\\\n",
    "> School of Computer Science, University of St Andrews\\\n",
    "> Contact: cs5052.staff@st-andrews.ac.uk\n",
    "\n",
    "This notebook introduces you to Spark programming using Python. Spark is a system that coordinates the processing of large datasets in parallel across many machines. In practice, you could run Spark across a cluster of nodes that will be managed by Spark. Spark in the context of the lab is installed and run on a single machine. \n",
    "\n",
    "You can setup the enviroment to run this notebook on the lab machine by: \n",
    "```\n",
    "cd <your desired folder>\n",
    "python3.12 -m venv pyspark\n",
    ". pyspark/bin/activate\n",
    "pip install --upgrade pip\n",
    "pip install pyspark jupyterlab\n",
    "```\n",
    "\n",
    "Run the JupyterLab server after activating the virtual environment using the following command:\n",
    "```\n",
    "jupyter-lab\n",
    "```\n",
    "A browser window should open automatically.\n",
    "\n",
    "To create self-contained notebooks, explicit commands must be provided in the code within the notebook for installing any additional packages using the following command:\n",
    "```\n",
    "%pip install <package_name>\n",
    "```\n",
    "\n",
    "**Note:** The notebook submitted for the CS5052 Practical 1 must run on the lab machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2e1dac",
   "metadata": {},
   "source": [
    "# `SparkSession`\n",
    "\n",
    "- Every Spark application consists of a driver program and executors (workers); see figure below\n",
    "- Driver program accesses Spark through a `SparkSession` object\n",
    "    - A unified point of entry as of Spark 2.0\n",
    "    - Represents a connection to a cluster\n",
    "    - `SparkContext`, `SQLContext` and `HiveContext` all combined in `SparkSession`\n",
    "\n",
    " ![Spark Overview; Obtained from: https://spark.apache.org/docs/latest/cluster-overview.html](images/sparksession.png)"
   ]
  },
  {
   "cell_type": "code",
   "id": "369588b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:17.079614Z",
     "start_time": "2026-02-13T19:46:17.077476Z"
    }
   },
   "source": [
    "# Import SparkSession class from pyspark.sql module\n",
    "# SparkSession is the entry point to Spark \n",
    "from pyspark.sql import SparkSession"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "c306afb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:20.030997Z",
     "start_time": "2026-02-13T19:46:17.125262Z"
    }
   },
   "source": [
    "# Create a SparkSession and assign it to variable 'spark'\n",
    "# There are different variants on the usage - refer to the documentation or a tutorial\n",
    "spark = SparkSession.builder.getOrCreate()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/13 19:46:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "ce864ecb",
   "metadata": {},
   "source": [
    "# DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56609aa0",
   "metadata": {},
   "source": [
    "## DataFrame: create manually"
   ]
  },
  {
   "cell_type": "code",
   "id": "974011f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:21.336607Z",
     "start_time": "2026-02-13T19:46:20.085563Z"
    }
   },
   "source": [
    "# Create a DataFrame with one column called “number” and 10000 rows\n",
    "data = spark.range(1000).toDF(\"number\")\n",
    "\n",
    "# Shows the first 20 rows by default\n",
    "data.show()\n",
    "\n",
    "# Show more or fewer rows N\n",
    "N = 50\n",
    "data.show(N)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "|    10|\n",
      "|    11|\n",
      "|    12|\n",
      "|    13|\n",
      "|    14|\n",
      "|    15|\n",
      "|    16|\n",
      "|    17|\n",
      "|    18|\n",
      "|    19|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "|    10|\n",
      "|    11|\n",
      "|    12|\n",
      "|    13|\n",
      "|    14|\n",
      "|    15|\n",
      "|    16|\n",
      "|    17|\n",
      "|    18|\n",
      "|    19|\n",
      "|    20|\n",
      "|    21|\n",
      "|    22|\n",
      "|    23|\n",
      "|    24|\n",
      "|    25|\n",
      "|    26|\n",
      "|    27|\n",
      "|    28|\n",
      "|    29|\n",
      "|    30|\n",
      "|    31|\n",
      "|    32|\n",
      "|    33|\n",
      "|    34|\n",
      "|    35|\n",
      "|    36|\n",
      "|    37|\n",
      "|    38|\n",
      "|    39|\n",
      "|    40|\n",
      "|    41|\n",
      "|    42|\n",
      "|    43|\n",
      "|    44|\n",
      "|    45|\n",
      "|    46|\n",
      "|    47|\n",
      "|    48|\n",
      "|    49|\n",
      "+------+\n",
      "only showing top 50 rows\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "55ac6cc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:22.235295Z",
     "start_time": "2026-02-13T19:46:21.392439Z"
    }
   },
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Python list containing two rows\n",
    "emp = [Row(\"Jack\", 24), Row(\"Bobby\", 26)]\n",
    "\n",
    "# Convert Python data into Spark DataFrame\n",
    "emp_df = spark.createDataFrame(emp, [\"name\",\"age\"])\n",
    "\n",
    "emp_df.show()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "| Jack| 24|\n",
      "|Bobby| 26|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "7750b44d",
   "metadata": {},
   "source": [
    "## DataFrame: create from file"
   ]
  },
  {
   "cell_type": "code",
   "id": "9b4cf528",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:22.802572Z",
     "start_time": "2026-02-13T19:46:22.288470Z"
    }
   },
   "source": [
    "# Create DataFrame from a file\n",
    "df = ( \n",
    "    spark.read\n",
    "    .option(\"header\", True)         # Tells Spark the first line is a header\n",
    "    .option(\"inferSchema\", True)    # Spark scans the column and infers data type; id will be an integer, country and capital will be a string\n",
    "    .format(\"csv\")                  \n",
    "    .load(\"sample_data1.csv\")\n",
    ")\n",
    "\n",
    "df.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------+\n",
      "| ID|       Country|    Capital|\n",
      "+---+--------------+-----------+\n",
      "|  1|        Canada|     Ottawa|\n",
      "|  2|        Mexico|Mexico City|\n",
      "|  3|        Brazil|   Brasilia|\n",
      "|  4|United Kingdom|     London|\n",
      "|  5|        France|      Paris|\n",
      "|  6|       Germany|     Berlin|\n",
      "|  7|         India|  New Delhi|\n",
      "|  8|         China|    Beijing|\n",
      "|  9|         Japan|      Tokyo|\n",
      "| 10|     Australia|   Canberra|\n",
      "+---+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "a2e8fe5a",
   "metadata": {},
   "source": [
    "## DataFrame: Datasource\n",
    "\n",
    "Many different file types are possible, including CSV, JSON, ORC, Parquet, Text, Table, JDBC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef0ceb4",
   "metadata": {},
   "source": [
    "# Two Major Operations\n",
    "\n",
    "- All abstractions such as RDD and DataFrames offer two types of operation\n",
    "    - **Transformation:** construct a new RDD/DataFrame from a previous one\n",
    "    - **Action:** compute the result based on an RDD/DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93597aad",
   "metadata": {},
   "source": [
    "# Transformations\n",
    "\n",
    "## Transformations: `printSchema()` and `describe()`"
   ]
  },
  {
   "cell_type": "code",
   "id": "64d12110",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:22.812715Z",
     "start_time": "2026-02-13T19:46:22.809498Z"
    }
   },
   "source": [
    "# Print the structure (data type of the columns) of the DataFrame\n",
    "df.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Capital: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "ee06ab7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:22.926744Z",
     "start_time": "2026-02-13T19:46:22.857148Z"
    }
   },
   "source": [
    "# Describes the schema of the DataFrame\n",
    "df.describe()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, ID: string, Country: string, Capital: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "e4517cce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:22.971762Z",
     "start_time": "2026-02-13T19:46:22.941896Z"
    }
   },
   "source": [
    "# Describe the structure of specific column\n",
    "df.select(\"Country\").describe()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, Country: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "e3dc155b",
   "metadata": {},
   "source": [
    "## Transformations: `where()` and `filter()`"
   ]
  },
  {
   "cell_type": "code",
   "id": "20c958ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:23.509934Z",
     "start_time": "2026-02-13T19:46:22.994109Z"
    }
   },
   "source": [
    "df_population = ( \n",
    "                    spark.read\n",
    "                    .option(\"header\", True)         \n",
    "                    .option(\"inferSchema\", True) \n",
    "                    .format(\"csv\")                  \n",
    "                    .load(\"sample_data2.csv\")\n",
    ")\n",
    "\n",
    "df_population.show()\n",
    "\n",
    "hundredK_plus = df_population.filter(\"Population >= 100000\")\n",
    "hundredK_plus.show()\n",
    "\n",
    "under_50K = df_population.where(\"Population <= 50000\")\n",
    "under_50K.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+----------+\n",
      "| ID|               Town|Population|\n",
      "+---+-------------------+----------+\n",
      "|  1|             London|   8982000|\n",
      "|  2|         Birmingham|   1141000|\n",
      "|  3|         Manchester|    553230|\n",
      "|  4|              Leeds|    789200|\n",
      "|  5|            Glasgow|    635640|\n",
      "|  6|Newcastle upon Tyne|    300820|\n",
      "|  7|            Bristol|    463400|\n",
      "|  8|          Sheffield|    584853|\n",
      "|  9|          Liverpool|    498042|\n",
      "| 10|          Cambridge|    125000|\n",
      "| 11|             Oxford|    154000|\n",
      "| 12|               Bath|     89000|\n",
      "| 13|               York|    209200|\n",
      "| 14|         Canterbury|     55000|\n",
      "| 15|          Lichfield|     34000|\n",
      "| 16|         St Andrews|     17000|\n",
      "| 17|          Inverness|     47000|\n",
      "| 18|           Stirling|     37000|\n",
      "| 19|           Aberdeen|    198000|\n",
      "| 20|             Dundee|    148000|\n",
      "+---+-------------------+----------+\n",
      "\n",
      "+---+-------------------+----------+\n",
      "| ID|               Town|Population|\n",
      "+---+-------------------+----------+\n",
      "|  1|             London|   8982000|\n",
      "|  2|         Birmingham|   1141000|\n",
      "|  3|         Manchester|    553230|\n",
      "|  4|              Leeds|    789200|\n",
      "|  5|            Glasgow|    635640|\n",
      "|  6|Newcastle upon Tyne|    300820|\n",
      "|  7|            Bristol|    463400|\n",
      "|  8|          Sheffield|    584853|\n",
      "|  9|          Liverpool|    498042|\n",
      "| 10|          Cambridge|    125000|\n",
      "| 11|             Oxford|    154000|\n",
      "| 13|               York|    209200|\n",
      "| 19|           Aberdeen|    198000|\n",
      "| 20|             Dundee|    148000|\n",
      "+---+-------------------+----------+\n",
      "\n",
      "+---+----------+----------+\n",
      "| ID|      Town|Population|\n",
      "+---+----------+----------+\n",
      "| 15| Lichfield|     34000|\n",
      "| 16|St Andrews|     17000|\n",
      "| 17| Inverness|     47000|\n",
      "| 18|  Stirling|     37000|\n",
      "+---+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "ce951e13",
   "metadata": {},
   "source": [
    "## Transformation: `distinct()` and\t`limit()` "
   ]
  },
  {
   "cell_type": "code",
   "id": "e95a04f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:24.064553Z",
     "start_time": "2026-02-13T19:46:23.516590Z"
    }
   },
   "source": [
    "df_town_village = ( \n",
    "                    spark.read\n",
    "                    .option(\"header\", True)         \n",
    "                    .option(\"inferSchema\", True) \n",
    "                    .format(\"csv\")                  \n",
    "                    .load(\"sample_data3.csv\")\n",
    ")\n",
    "\n",
    "df_town_village.show()\n",
    "\n",
    "unique_county = df_town_village.select(\"County\").distinct()\n",
    "unique_county.show()\n",
    "\n",
    "N = 5\n",
    "shortN_list = df_town_village.limit(N)\n",
    "shortN_list.show()\n",
    "\n",
    "# Alternate usage\n",
    "df_town_village.limit(N).show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|              County|  Town/Village|\n",
      "+--------------------+--------------+\n",
      "|       Aberdeenshire|      Aberdeen|\n",
      "|       Aberdeenshire|         Banff|\n",
      "|       Aberdeenshire|     Peterhead|\n",
      "|       Aberdeenshire|      Banchory|\n",
      "|                Fife|    St Andrews|\n",
      "|                Fife|         Cupar|\n",
      "|                Fife|    Anstruther|\n",
      "|                Fife|     Kirkcaldy|\n",
      "|            Highland|     Inverness|\n",
      "|            Highland|  Fort William|\n",
      "|            Highland|     Kingussie|\n",
      "|            Highland|        Thurso|\n",
      "|Dumfries and Gall...|      Dumfries|\n",
      "|Dumfries and Gall...|Castle Douglas|\n",
      "|Dumfries and Gall...|     Stranraer|\n",
      "|   Perth and Kinross|         Perth|\n",
      "|   Perth and Kinross|        Crieff|\n",
      "|   Perth and Kinross|   Blairgowrie|\n",
      "|            Stirling|      Stirling|\n",
      "|            Stirling|     Callander|\n",
      "+--------------------+--------------+\n",
      "\n",
      "+--------------------+\n",
      "|              County|\n",
      "+--------------------+\n",
      "|            Highland|\n",
      "|Dumfries and Gall...|\n",
      "|   Perth and Kinross|\n",
      "|                Fife|\n",
      "|       Aberdeenshire|\n",
      "|            Stirling|\n",
      "+--------------------+\n",
      "\n",
      "+-------------+------------+\n",
      "|       County|Town/Village|\n",
      "+-------------+------------+\n",
      "|Aberdeenshire|    Aberdeen|\n",
      "|Aberdeenshire|       Banff|\n",
      "|Aberdeenshire|   Peterhead|\n",
      "|Aberdeenshire|    Banchory|\n",
      "|         Fife|  St Andrews|\n",
      "+-------------+------------+\n",
      "\n",
      "+-------------+------------+\n",
      "|       County|Town/Village|\n",
      "+-------------+------------+\n",
      "|Aberdeenshire|    Aberdeen|\n",
      "|Aberdeenshire|       Banff|\n",
      "|Aberdeenshire|   Peterhead|\n",
      "|Aberdeenshire|    Banchory|\n",
      "|         Fife|  St Andrews|\n",
      "+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "89a3dc99",
   "metadata": {},
   "source": [
    "## Transformation: Sorting using `sort()` or `orderBy()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992b324c",
   "metadata": {},
   "source": [
    "### Basic sorting"
   ]
  },
  {
   "cell_type": "code",
   "id": "5ab48fd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:24.216733Z",
     "start_time": "2026-02-13T19:46:24.070274Z"
    }
   },
   "source": [
    "# Sort by a single column\n",
    "sorted = df_town_village.sort(\"County\")\n",
    "sorted.show()\n",
    "\n",
    "# Sort by multiple columns\n",
    "sorted = df_town_village.sort(\"County\", \"Town/Village\")\n",
    "sorted.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|              County|  Town/Village|\n",
      "+--------------------+--------------+\n",
      "|       Aberdeenshire|      Aberdeen|\n",
      "|       Aberdeenshire|         Banff|\n",
      "|       Aberdeenshire|     Peterhead|\n",
      "|       Aberdeenshire|      Banchory|\n",
      "|Dumfries and Gall...|      Dumfries|\n",
      "|Dumfries and Gall...|Castle Douglas|\n",
      "|Dumfries and Gall...|     Stranraer|\n",
      "|                Fife|    St Andrews|\n",
      "|                Fife|         Cupar|\n",
      "|                Fife|    Anstruther|\n",
      "|                Fife|     Kirkcaldy|\n",
      "|            Highland|     Inverness|\n",
      "|            Highland|  Fort William|\n",
      "|            Highland|     Kingussie|\n",
      "|            Highland|        Thurso|\n",
      "|   Perth and Kinross|         Perth|\n",
      "|   Perth and Kinross|        Crieff|\n",
      "|   Perth and Kinross|   Blairgowrie|\n",
      "|            Stirling|      Stirling|\n",
      "|            Stirling|     Callander|\n",
      "+--------------------+--------------+\n",
      "\n",
      "+--------------------+--------------+\n",
      "|              County|  Town/Village|\n",
      "+--------------------+--------------+\n",
      "|       Aberdeenshire|      Aberdeen|\n",
      "|       Aberdeenshire|      Banchory|\n",
      "|       Aberdeenshire|         Banff|\n",
      "|       Aberdeenshire|     Peterhead|\n",
      "|Dumfries and Gall...|Castle Douglas|\n",
      "|Dumfries and Gall...|      Dumfries|\n",
      "|Dumfries and Gall...|     Stranraer|\n",
      "|                Fife|    Anstruther|\n",
      "|                Fife|         Cupar|\n",
      "|                Fife|     Kirkcaldy|\n",
      "|                Fife|    St Andrews|\n",
      "|            Highland|  Fort William|\n",
      "|            Highland|     Inverness|\n",
      "|            Highland|     Kingussie|\n",
      "|            Highland|        Thurso|\n",
      "|   Perth and Kinross|   Blairgowrie|\n",
      "|   Perth and Kinross|        Crieff|\n",
      "|   Perth and Kinross|         Perth|\n",
      "|            Stirling|     Callander|\n",
      "|            Stirling|      Stirling|\n",
      "+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "815e9fe0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:24.323736Z",
     "start_time": "2026-02-13T19:46:24.221846Z"
    }
   },
   "source": [
    "# Order by a single column\n",
    "sorted = df_town_village.orderBy(\"Town/Village\")\n",
    "sorted.show()\n",
    "\n",
    "# Order by multiple columns\n",
    "sorted = df_town_village.orderBy(\"County\", \"Town/Village\")\n",
    "sorted.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|              County|  Town/Village|\n",
      "+--------------------+--------------+\n",
      "|       Aberdeenshire|      Aberdeen|\n",
      "|                Fife|    Anstruther|\n",
      "|       Aberdeenshire|      Banchory|\n",
      "|       Aberdeenshire|         Banff|\n",
      "|   Perth and Kinross|   Blairgowrie|\n",
      "|            Stirling|     Callander|\n",
      "|Dumfries and Gall...|Castle Douglas|\n",
      "|   Perth and Kinross|        Crieff|\n",
      "|                Fife|         Cupar|\n",
      "|Dumfries and Gall...|      Dumfries|\n",
      "|            Highland|  Fort William|\n",
      "|            Highland|     Inverness|\n",
      "|            Highland|     Kingussie|\n",
      "|                Fife|     Kirkcaldy|\n",
      "|   Perth and Kinross|         Perth|\n",
      "|       Aberdeenshire|     Peterhead|\n",
      "|                Fife|    St Andrews|\n",
      "|            Stirling|      Stirling|\n",
      "|Dumfries and Gall...|     Stranraer|\n",
      "|            Highland|        Thurso|\n",
      "+--------------------+--------------+\n",
      "\n",
      "+--------------------+--------------+\n",
      "|              County|  Town/Village|\n",
      "+--------------------+--------------+\n",
      "|       Aberdeenshire|      Aberdeen|\n",
      "|       Aberdeenshire|      Banchory|\n",
      "|       Aberdeenshire|         Banff|\n",
      "|       Aberdeenshire|     Peterhead|\n",
      "|Dumfries and Gall...|Castle Douglas|\n",
      "|Dumfries and Gall...|      Dumfries|\n",
      "|Dumfries and Gall...|     Stranraer|\n",
      "|                Fife|    Anstruther|\n",
      "|                Fife|         Cupar|\n",
      "|                Fife|     Kirkcaldy|\n",
      "|                Fife|    St Andrews|\n",
      "|            Highland|  Fort William|\n",
      "|            Highland|     Inverness|\n",
      "|            Highland|     Kingussie|\n",
      "|            Highland|        Thurso|\n",
      "|   Perth and Kinross|   Blairgowrie|\n",
      "|   Perth and Kinross|        Crieff|\n",
      "|   Perth and Kinross|         Perth|\n",
      "|            Stirling|     Callander|\n",
      "|            Stirling|      Stirling|\n",
      "+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "5e35e3a3",
   "metadata": {},
   "source": [
    "### Specifying sort direction"
   ]
  },
  {
   "cell_type": "code",
   "id": "fafa0363",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:24.451947Z",
     "start_time": "2026-02-13T19:46:24.333071Z"
    }
   },
   "source": [
    "from pyspark.sql.functions import desc, asc \n",
    "\n",
    "sorted = df_town_village.orderBy(desc(\"Town/Village\"))\n",
    "sorted.show()\n",
    "\n",
    "sorted = df_town_village.orderBy(asc(\"County\"), desc(\"Town/Village\"))\n",
    "sorted.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|              County|  Town/Village|\n",
      "+--------------------+--------------+\n",
      "|            Highland|        Thurso|\n",
      "|Dumfries and Gall...|     Stranraer|\n",
      "|            Stirling|      Stirling|\n",
      "|                Fife|    St Andrews|\n",
      "|       Aberdeenshire|     Peterhead|\n",
      "|   Perth and Kinross|         Perth|\n",
      "|                Fife|     Kirkcaldy|\n",
      "|            Highland|     Kingussie|\n",
      "|            Highland|     Inverness|\n",
      "|            Highland|  Fort William|\n",
      "|Dumfries and Gall...|      Dumfries|\n",
      "|                Fife|         Cupar|\n",
      "|   Perth and Kinross|        Crieff|\n",
      "|Dumfries and Gall...|Castle Douglas|\n",
      "|            Stirling|     Callander|\n",
      "|   Perth and Kinross|   Blairgowrie|\n",
      "|       Aberdeenshire|         Banff|\n",
      "|       Aberdeenshire|      Banchory|\n",
      "|                Fife|    Anstruther|\n",
      "|       Aberdeenshire|      Aberdeen|\n",
      "+--------------------+--------------+\n",
      "\n",
      "+--------------------+--------------+\n",
      "|              County|  Town/Village|\n",
      "+--------------------+--------------+\n",
      "|       Aberdeenshire|     Peterhead|\n",
      "|       Aberdeenshire|         Banff|\n",
      "|       Aberdeenshire|      Banchory|\n",
      "|       Aberdeenshire|      Aberdeen|\n",
      "|Dumfries and Gall...|     Stranraer|\n",
      "|Dumfries and Gall...|      Dumfries|\n",
      "|Dumfries and Gall...|Castle Douglas|\n",
      "|                Fife|    St Andrews|\n",
      "|                Fife|     Kirkcaldy|\n",
      "|                Fife|         Cupar|\n",
      "|                Fife|    Anstruther|\n",
      "|            Highland|        Thurso|\n",
      "|            Highland|     Kingussie|\n",
      "|            Highland|     Inverness|\n",
      "|            Highland|  Fort William|\n",
      "|   Perth and Kinross|         Perth|\n",
      "|   Perth and Kinross|        Crieff|\n",
      "|   Perth and Kinross|   Blairgowrie|\n",
      "|            Stirling|      Stirling|\n",
      "|            Stirling|     Callander|\n",
      "+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "170dd105",
   "metadata": {},
   "source": [
    "## Transformation: Sampling data using `sample`"
   ]
  },
  {
   "cell_type": "code",
   "id": "d3bef7a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:24.518497Z",
     "start_time": "2026-02-13T19:46:24.461520Z"
    }
   },
   "source": [
    "with_replacement = False    # Sample without replacement; each row can appear at most once \n",
    "fraction = 0.50             # Roughly 50% of the rows are selected\n",
    "seed = None                 # Sets the random seed for reproducibility; if an integer sample value is set it produces the same sample everytime \n",
    "\n",
    "sample = df_town_village.sample(with_replacement, fraction, seed)\n",
    "sample.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|              County|  Town/Village|\n",
      "+--------------------+--------------+\n",
      "|       Aberdeenshire|         Banff|\n",
      "|                Fife|         Cupar|\n",
      "|                Fife|    Anstruther|\n",
      "|            Highland|     Inverness|\n",
      "|            Highland|        Thurso|\n",
      "|Dumfries and Gall...|Castle Douglas|\n",
      "|Dumfries and Gall...|     Stranraer|\n",
      "|   Perth and Kinross|   Blairgowrie|\n",
      "|            Stirling|      Stirling|\n",
      "+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "738e0474",
   "metadata": {},
   "source": [
    "## Transformation: Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "id": "e4d49bf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:24.789608Z",
     "start_time": "2026-02-13T19:46:24.530867Z"
    }
   },
   "source": [
    "from pyspark.sql.functions import count, countDistinct \n",
    "\n",
    "df_town_village.select(count(\"County\")).show()\n",
    "\n",
    "df_town_village.select(countDistinct(\"County\")).show()\n",
    "\n",
    "# min, max, avg, first, last and groupBy functions are available and self explanatory\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|count(County)|\n",
      "+-------------+\n",
      "|           20|\n",
      "+-------------+\n",
      "\n",
      "+----------------------+\n",
      "|count(DISTINCT County)|\n",
      "+----------------------+\n",
      "|                     6|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "7114d6da",
   "metadata": {},
   "source": [
    "## DataFrame: Some Actions"
   ]
  },
  {
   "cell_type": "code",
   "id": "3458c7e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:25.064101Z",
     "start_time": "2026-02-13T19:46:24.797613Z"
    }
   },
   "source": [
    "# first()\n",
    "row = df_town_village.first()\n",
    "print(row)\n",
    "print(row[\"Town/Village\"])      #Access column of the first row\n",
    "\n",
    "# show()\n",
    "df_town_village.show()\n",
    "N = 6\n",
    "df_town_village.show(N)\n",
    "\n",
    "# take(N)\n",
    "N = 4\n",
    "rows = df_town_village.take(N)  #Similar to first, but returns multiple rows\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# collect()\n",
    "all_rows = df_town_village.collect()    #Returns all rows as a list of objects\n",
    "# Note: if the DataFrame is large, then may not work as all memory is brought into memory\n",
    "# Use this for small datasets or debugging\n",
    "for row in all_rows:\n",
    "    print(row)\n",
    "\n",
    "#count()\n",
    "print(f\"Total rows: {df_town_village.count()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(County='Aberdeenshire', Town/Village='Aberdeen')\n",
      "Aberdeen\n",
      "+--------------------+--------------+\n",
      "|              County|  Town/Village|\n",
      "+--------------------+--------------+\n",
      "|       Aberdeenshire|      Aberdeen|\n",
      "|       Aberdeenshire|         Banff|\n",
      "|       Aberdeenshire|     Peterhead|\n",
      "|       Aberdeenshire|      Banchory|\n",
      "|                Fife|    St Andrews|\n",
      "|                Fife|         Cupar|\n",
      "|                Fife|    Anstruther|\n",
      "|                Fife|     Kirkcaldy|\n",
      "|            Highland|     Inverness|\n",
      "|            Highland|  Fort William|\n",
      "|            Highland|     Kingussie|\n",
      "|            Highland|        Thurso|\n",
      "|Dumfries and Gall...|      Dumfries|\n",
      "|Dumfries and Gall...|Castle Douglas|\n",
      "|Dumfries and Gall...|     Stranraer|\n",
      "|   Perth and Kinross|         Perth|\n",
      "|   Perth and Kinross|        Crieff|\n",
      "|   Perth and Kinross|   Blairgowrie|\n",
      "|            Stirling|      Stirling|\n",
      "|            Stirling|     Callander|\n",
      "+--------------------+--------------+\n",
      "\n",
      "+-------------+------------+\n",
      "|       County|Town/Village|\n",
      "+-------------+------------+\n",
      "|Aberdeenshire|    Aberdeen|\n",
      "|Aberdeenshire|       Banff|\n",
      "|Aberdeenshire|   Peterhead|\n",
      "|Aberdeenshire|    Banchory|\n",
      "|         Fife|  St Andrews|\n",
      "|         Fife|       Cupar|\n",
      "+-------------+------------+\n",
      "only showing top 6 rows\n",
      "Row(County='Aberdeenshire', Town/Village='Aberdeen')\n",
      "Row(County='Aberdeenshire', Town/Village='Banff')\n",
      "Row(County='Aberdeenshire', Town/Village='Peterhead')\n",
      "Row(County='Aberdeenshire', Town/Village='Banchory')\n",
      "Row(County='Aberdeenshire', Town/Village='Aberdeen')\n",
      "Row(County='Aberdeenshire', Town/Village='Banff')\n",
      "Row(County='Aberdeenshire', Town/Village='Peterhead')\n",
      "Row(County='Aberdeenshire', Town/Village='Banchory')\n",
      "Row(County='Fife', Town/Village='St Andrews')\n",
      "Row(County='Fife', Town/Village='Cupar')\n",
      "Row(County='Fife', Town/Village='Anstruther')\n",
      "Row(County='Fife', Town/Village='Kirkcaldy')\n",
      "Row(County='Highland', Town/Village='Inverness')\n",
      "Row(County='Highland', Town/Village='Fort William')\n",
      "Row(County='Highland', Town/Village='Kingussie')\n",
      "Row(County='Highland', Town/Village='Thurso')\n",
      "Row(County='Dumfries and Galloway', Town/Village='Dumfries')\n",
      "Row(County='Dumfries and Galloway', Town/Village='Castle Douglas')\n",
      "Row(County='Dumfries and Galloway', Town/Village='Stranraer')\n",
      "Row(County='Perth and Kinross', Town/Village='Perth')\n",
      "Row(County='Perth and Kinross', Town/Village='Crieff')\n",
      "Row(County='Perth and Kinross', Town/Village='Blairgowrie')\n",
      "Row(County='Stirling', Town/Village='Stirling')\n",
      "Row(County='Stirling', Town/Village='Callander')\n",
      "Total rows: 20\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "8861f3f1",
   "metadata": {},
   "source": [
    "# RDD\n",
    "\n",
    "- Low level but still relevant in some cases:\n",
    "- Raw data processing e.g. text file without structure.\n",
    "    - Creating new RDDs\n",
    "    - Transforming existing RDDs\n",
    "    - Computing results from RDDs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fe500b",
   "metadata": {},
   "source": [
    "## Create RDD"
   ]
  },
  {
   "cell_type": "code",
   "id": "846fcfe1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:25.154493Z",
     "start_time": "2026-02-13T19:46:25.069433Z"
    }
   },
   "source": [
    "# From an existing file\n",
    "lines = spark.sparkContext.textFile(\"README_dummy.md\")\n",
    "\n",
    "# or\n",
    "sc = spark.sparkContext\n",
    "lines = sc.textFile(\"README_dummy.md\")\n",
    "\n",
    "# Collect all lines into a Python list\n",
    "all_lines = lines.collect()\n",
    "\n",
    "# Print each line\n",
    "for line in all_lines:\n",
    "    print(line)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Spark Sample Project\n",
      "**Note:** this file will be used in some of the examples shown in `spark-test.ipynb`\n",
      "\n",
      "This is a **dummy project** to demonstrate working with **Apache Spark** using Python (`pyspark`).  \n",
      "It contains some sample data, Spark DataFrame examples, and basic transformations.\n",
      "\n",
      "---\n",
      "\n",
      "## Overview\n",
      "\n",
      "Apache Spark is a **distributed computing framework** that allows processing of large datasets across multiple machines.  \n",
      "It provides APIs for Python, Scala, Java, and R, with built-in support for SQL, machine learning, and streaming.\n",
      "\n",
      "In this project, we will use **Spark DataFrames** to handle tabular data efficiently.\n",
      "\n",
      "---\n",
      "\n",
      "## Sample Spark Operations\n",
      "\n",
      "```python\n",
      "from pyspark.sql import SparkSession\n",
      "from pyspark.sql.functions import col\n",
      "\n",
      "# Create Spark session\n",
      "spark = SparkSession.builder.appName(\"DummyProject\").getOrCreate()\n",
      "\n",
      "# Load sample CSV\n",
      "df = spark.read.option(\"header\", True).csv(\"sample_data.csv\")\n",
      "\n",
      "# Show first 5 rows\n",
      "df.show(5)\n",
      "\n",
      "# Filter rows using Spark\n",
      "high_population = df.filter(col(\"population\") > 100000)\n",
      "high_population.show()\n",
      "\n",
      "# Count total rows\n",
      "total_rows = df.count()\n",
      "print(f\"Total rows in DataFrame: {total_rows}\")\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "8a299b4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:25.199251Z",
     "start_time": "2026-02-13T19:46:25.177476Z"
    }
   },
   "source": [
    "# From a list\n",
    "numbers = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "# numbers is an RDD containing numbers 1 to 8\n",
    "# The data is split into partitions and can be processed in parallel\n",
    "\n",
    "# Aggregate the partitions and print all numbers\n",
    "print(numbers.collect())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "94f78071",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:25.380347Z",
     "start_time": "2026-02-13T19:46:25.226530Z"
    }
   },
   "source": [
    "# Create an RDD from a file \n",
    "lines = sc.textFile(\"README_dummy.md\")\n",
    "\n",
    "# Create new RDD with lines containing Spark\n",
    "lines = lines.filter(lambda x: 'Spark' in x)\n",
    "\n",
    "# Count the number of items in this RDD\n",
    "# Note: The above two lines doesn't do anything \n",
    "# The statement below will read the file and do the computation\n",
    "print(lines.count())\n",
    "\n",
    "# The statement below will read the file and do the computation again\n",
    "print(lines.count())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "200690c4",
   "metadata": {},
   "source": [
    "## RDD - Persisting\n",
    "\n",
    "- Spark recomputes RDDs each time an action is performed on it\n",
    "    - By default RDD is not stored in memory\n",
    "    - The `persist()` function stores an RDD permanently"
   ]
  },
  {
   "cell_type": "code",
   "id": "d479898f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:25.689002Z",
     "start_time": "2026-02-13T19:46:25.431644Z"
    }
   },
   "source": [
    "lines = sc.textFile(\"README_dummy.md\")\n",
    "lines = lines.filter(lambda x: 'Spark' in x)\n",
    "\n",
    "# Load and store dataset in memory\n",
    "lines.persist()\n",
    "\n",
    "# Perform computation on the stored dataset\n",
    "print(lines.count())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "b0cb9faf",
   "metadata": {},
   "source": [
    "## Basic RDD Transformation Functions\n",
    "\n",
    "- Construct an RDD from a previous one\n",
    "    - Performed on one or more RDDs\n",
    "    - Return a new RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74bfbfc",
   "metadata": {},
   "source": [
    "### `filter()`\n",
    "- Takes in a function, returns an RDD that only has elements that pass the filter() function"
   ]
  },
  {
   "cell_type": "code",
   "id": "a1a6e708",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:25.769915Z",
     "start_time": "2026-02-13T19:46:25.696170Z"
    }
   },
   "source": [
    "lines = sc.textFile(\"README_dummy.md\")\n",
    "\n",
    "# Create a new RDD consisting lines that contain ‘Spark’\n",
    "lines = lines.filter(lambda x: 'Spark' in x)\n",
    "\n",
    "all_lines = lines.collect()\n",
    "\n",
    "for line in all_lines:\n",
    "    print(line)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Spark Sample Project\n",
      "This is a **dummy project** to demonstrate working with **Apache Spark** using Python (`pyspark`).  \n",
      "It contains some sample data, Spark DataFrame examples, and basic transformations.\n",
      "Apache Spark is a **distributed computing framework** that allows processing of large datasets across multiple machines.  \n",
      "In this project, we will use **Spark DataFrames** to handle tabular data efficiently.\n",
      "## Sample Spark Operations\n",
      "from pyspark.sql import SparkSession\n",
      "# Create Spark session\n",
      "spark = SparkSession.builder.appName(\"DummyProject\").getOrCreate()\n",
      "# Filter rows using Spark\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "e7c319b1",
   "metadata": {},
   "source": [
    "### `map()`\n",
    "- Takes in a function and applies it to each element in the RDD"
   ]
  },
  {
   "cell_type": "code",
   "id": "3b415d0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:25.876452Z",
     "start_time": "2026-02-13T19:46:25.778161Z"
    }
   },
   "source": [
    "lines = sc.textFile(\"README_dummy.md\")\n",
    "\n",
    "# Create a new RDD in which all strings are in uppercase\n",
    "lines = lines.map(lambda x: x.upper())\n",
    "\n",
    "all_lines = lines.collect()\n",
    "\n",
    "for line in all_lines:\n",
    "    print(line)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# SPARK SAMPLE PROJECT\n",
      "**NOTE:** THIS FILE WILL BE USED IN SOME OF THE EXAMPLES SHOWN IN `SPARK-TEST.IPYNB`\n",
      "\n",
      "THIS IS A **DUMMY PROJECT** TO DEMONSTRATE WORKING WITH **APACHE SPARK** USING PYTHON (`PYSPARK`).  \n",
      "IT CONTAINS SOME SAMPLE DATA, SPARK DATAFRAME EXAMPLES, AND BASIC TRANSFORMATIONS.\n",
      "\n",
      "---\n",
      "\n",
      "## OVERVIEW\n",
      "\n",
      "APACHE SPARK IS A **DISTRIBUTED COMPUTING FRAMEWORK** THAT ALLOWS PROCESSING OF LARGE DATASETS ACROSS MULTIPLE MACHINES.  \n",
      "IT PROVIDES APIS FOR PYTHON, SCALA, JAVA, AND R, WITH BUILT-IN SUPPORT FOR SQL, MACHINE LEARNING, AND STREAMING.\n",
      "\n",
      "IN THIS PROJECT, WE WILL USE **SPARK DATAFRAMES** TO HANDLE TABULAR DATA EFFICIENTLY.\n",
      "\n",
      "---\n",
      "\n",
      "## SAMPLE SPARK OPERATIONS\n",
      "\n",
      "```PYTHON\n",
      "FROM PYSPARK.SQL IMPORT SPARKSESSION\n",
      "FROM PYSPARK.SQL.FUNCTIONS IMPORT COL\n",
      "\n",
      "# CREATE SPARK SESSION\n",
      "SPARK = SPARKSESSION.BUILDER.APPNAME(\"DUMMYPROJECT\").GETORCREATE()\n",
      "\n",
      "# LOAD SAMPLE CSV\n",
      "DF = SPARK.READ.OPTION(\"HEADER\", TRUE).CSV(\"SAMPLE_DATA.CSV\")\n",
      "\n",
      "# SHOW FIRST 5 ROWS\n",
      "DF.SHOW(5)\n",
      "\n",
      "# FILTER ROWS USING SPARK\n",
      "HIGH_POPULATION = DF.FILTER(COL(\"POPULATION\") > 100000)\n",
      "HIGH_POPULATION.SHOW()\n",
      "\n",
      "# COUNT TOTAL ROWS\n",
      "TOTAL_ROWS = DF.COUNT()\n",
      "PRINT(F\"TOTAL ROWS IN DATAFRAME: {TOTAL_ROWS}\")\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "99285319",
   "metadata": {},
   "source": [
    "### `flatmap()`\n",
    "- Applies a function to each element in an RDD\n",
    "- Returns a sequence (list of elements)\n",
    "- The final RDD is flattened"
   ]
  },
  {
   "cell_type": "code",
   "id": "d5cbc518",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:26.005423Z",
     "start_time": "2026-02-13T19:46:25.915850Z"
    }
   },
   "source": [
    "lines = sc.parallelize([\n",
    "    \"I love Spark\",\n",
    "    \"Spark is awesome\",\n",
    "    \"Big data rocks\"\n",
    "])\n",
    "\n",
    "words_using_map = lines.map(lambda line: line.split(\" \"))\n",
    "print(words_using_map.collect())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'love', 'Spark'], ['Spark', 'is', 'awesome'], ['Big', 'data', 'rocks']]\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "26194aad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:26.099038Z",
     "start_time": "2026-02-13T19:46:26.018047Z"
    }
   },
   "source": [
    "words_using_flatmap = lines.flatMap(lambda line: line.split(\" \"))\n",
    "print(words_using_flatmap.collect())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'Spark', 'Spark', 'is', 'awesome', 'Big', 'data', 'rocks']\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "f03683f6",
   "metadata": {},
   "source": [
    "### `distinct()`\n",
    "- Returns a new RDD with only distinct items"
   ]
  },
  {
   "cell_type": "code",
   "id": "21054f0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:26.306663Z",
     "start_time": "2026-02-13T19:46:26.105486Z"
    }
   },
   "source": [
    "numbers = sc.parallelize([0, 1, 2, 4, 7, 5, 4, 3, 2, 1, 1, 0])\n",
    "numbers = numbers.distinct()\n",
    "print(numbers.collect())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 7]\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "e9720738",
   "metadata": {},
   "source": [
    "### `union(other)`\n",
    "- Returns a new RDD consisting of items from both sources"
   ]
  },
  {
   "cell_type": "code",
   "id": "3196c6eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:26.350145Z",
     "start_time": "2026-02-13T19:46:26.316384Z"
    }
   },
   "source": [
    "numbers = sc.parallelize([0, 1, 2, 3, 4])\n",
    "characters = sc.parallelize(['A', 'B', 'C', 'D', 'E'])\n",
    "result = numbers.union(characters)\n",
    "print(result.collect())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 'A', 'B', 'C', 'D', 'E']\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "e22e8ea3",
   "metadata": {},
   "source": [
    "### `intersection(other)`\n",
    "- Returns a new RDD consisting of only items from both sources and removes all duplicates"
   ]
  },
  {
   "cell_type": "code",
   "id": "4c6bc156",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:26.796366Z",
     "start_time": "2026-02-13T19:46:26.378213Z"
    }
   },
   "source": [
    "number_list1 = sc.parallelize([0, 1, 2, 4, 6, 7, 8])\n",
    "number_list2 = sc.parallelize([0, 1, 3, 4, 5, 7])\n",
    "result = number_list1.intersection(number_list2)\n",
    "print(result.collect())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 4, 7]\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "d14cf11e",
   "metadata": {},
   "source": [
    "### `subtract(other)`\n",
    "- Returns a new RDD consisting of only items in the first RDD but not in the other one"
   ]
  },
  {
   "cell_type": "code",
   "id": "4fb18594",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:27.197757Z",
     "start_time": "2026-02-13T19:46:26.804243Z"
    }
   },
   "source": [
    "number_list1 = sc.parallelize([0, 1, 2, 4, 6, 7, 8])\n",
    "number_list2 = sc.parallelize([0, 1, 3, 4, 5, 7])\n",
    "result = number_list1.subtract(number_list2)\n",
    "print(result.collect())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 6, 8]\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "4f688102",
   "metadata": {},
   "source": [
    "## Basic RDD Action Functions\n",
    "\n",
    "- Compute result based on RDD(s)\n",
    "    - Performed on one or more RDD(s)\n",
    "    - Return a result, which is not an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aaf4e5",
   "metadata": {},
   "source": [
    "### `first()`\n",
    "- Returns the first item in an RDD"
   ]
  },
  {
   "cell_type": "code",
   "id": "9f8ac995",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:27.321620Z",
     "start_time": "2026-02-13T19:46:27.204439Z"
    }
   },
   "source": [
    "numbers = sc.parallelize([0, 1, 2, 3, 4, 5, 6, 7, 8]) \n",
    "print(numbers.first())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "id": "3b64e2b3",
   "metadata": {},
   "source": [
    "### `collect()`\n",
    "- Returns a list containing the entire RDD's content"
   ]
  },
  {
   "cell_type": "code",
   "id": "2be6d975",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:27.367887Z",
     "start_time": "2026-02-13T19:46:27.339458Z"
    }
   },
   "source": [
    "numbers = sc.parallelize([0, 1, 2, 3, 4, 5, 6, 7, 8]) \n",
    "print(numbers.collect())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "95c7cdb2",
   "metadata": {},
   "source": [
    "### `count()`\n",
    "- Returns the number of items in an RDD"
   ]
  },
  {
   "cell_type": "code",
   "id": "e86dcef1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:27.476148Z",
     "start_time": "2026-02-13T19:46:27.392033Z"
    }
   },
   "source": [
    "numbers = sc.parallelize([0, 1, 2, 3, 4, 5, 6, 7, 8]) \n",
    "print(numbers.count())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "id": "47196a9e",
   "metadata": {},
   "source": [
    "### `reduce(function)`\n",
    "- Takes a function that operates on two elements and returns a new element"
   ]
  },
  {
   "cell_type": "code",
   "id": "0c9548b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:27.592353Z",
     "start_time": "2026-02-13T19:46:27.486239Z"
    }
   },
   "source": [
    "numbers = sc.parallelize([1, 2, 3, 4, 5]) \n",
    "result = numbers.reduce(lambda x, y: x * y)\n",
    "print(result)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "id": "be774f67",
   "metadata": {},
   "source": [
    "### `takeOrdered(num, ordering)`\n",
    "- Return a number of items based on the provided ordering"
   ]
  },
  {
   "cell_type": "code",
   "id": "a08f1183",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T19:46:27.766238Z",
     "start_time": "2026-02-13T19:46:27.599651Z"
    }
   },
   "source": [
    "numbers = sc.parallelize([8, 0, 4, 6, 9, 7, 2, 1, 5, 3])\n",
    "\n",
    "# Return five smallest numbers from the list\n",
    "print(numbers.takeOrdered(5, lambda x: x))\n",
    "\n",
    "# Return five largest numbers from the list\n",
    "print(numbers.takeOrdered(5, lambda x: -x))\n",
    "# Note: How this function works:\n",
    "# Original numbers: 8, 0, 4, 6, 9, 7, 2, 1, 5, 3\n",
    "# Negated numbers: -8, 0, -4, -6, -9, -7, -2, -1, -5, -3\n",
    "# 5 smallest of these: -9, -8, -7, -6, -5\n",
    "# Negate back: 9, 8, 7, 6, 5"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[9, 8, 7, 6, 5]\n"
     ]
    }
   ],
   "execution_count": 34
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
