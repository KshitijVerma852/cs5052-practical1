{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "486b40ec",
   "metadata": {},
   "source": [
    "# CS5052 - Spark Programming\n",
    "> Created by: Professor Blesson Varghese\\\n",
    "> School of Computer Science, University of St Andrews\\\n",
    "> Contact: cs5052.staff@st-andrews.ac.uk\n",
    "\n",
    "This notebook introduces you to Spark programming using Python. Spark is a system that coordinates the processing of large datasets in parallel across many machines. In practice, you could run Spark across a cluster of nodes that will be managed by Spark. Spark in the context of the lab is installed and run on a single machine. \n",
    "\n",
    "You can setup the enviroment to run this notebook on the lab machine by: \n",
    "```\n",
    "cd <your desired folder>\n",
    "python3.12 -m venv pyspark\n",
    ". pyspark/bin/activate\n",
    "pip install --upgrade pip\n",
    "pip install pyspark jupyterlab\n",
    "```\n",
    "\n",
    "Run the JupyterLab server after activating the virtual environment using the following command:\n",
    "```\n",
    "jupyter-lab\n",
    "```\n",
    "A browser window should open automatically.\n",
    "\n",
    "To create self-contained notebooks, explicit commands must be provided in the code within the notebook for installing any additional packages using the following command:\n",
    "```\n",
    "%pip install <package_name>\n",
    "```\n",
    "\n",
    "**Note:** The notebook submitted for the CS5052 Practical 1 must run on the lab machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2e1dac",
   "metadata": {},
   "source": [
    "# `SparkSession`\n",
    "\n",
    "- Every Spark application consists of a driver program and executors (workers); see figure below\n",
    "- Driver program accesses Spark through a `SparkSession` object\n",
    "    - A unified point of entry as of Spark 2.0\n",
    "    - Represents a connection to a cluster\n",
    "    - `SparkContext`, `SQLContext` and `HiveContext` all combined in `SparkSession`\n",
    "\n",
    " ![Spark Overview; Obtained from: https://spark.apache.org/docs/latest/cluster-overview.html](images/sparksession.png)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T22:31:50.748074400Z",
     "start_time": "2026-02-13T22:31:50.637482500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import os\n",
    "# import sys\n",
    "#\n",
    "# print(\"Python being used:\", sys.executable)\n",
    "#\n",
    "# os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "# os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
   ],
   "id": "ee2c46e925ab0a4b",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "369588b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T22:31:51.039975300Z",
     "start_time": "2026-02-13T22:31:50.782168200Z"
    }
   },
   "source": [
    "# Import SparkSession class from pyspark.sql module\n",
    "# SparkSession is the entry point to Spark \n",
    "from pyspark.sql import SparkSession"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "c306afb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T22:32:35.321866100Z",
     "start_time": "2026-02-13T22:31:51.105000900Z"
    }
   },
   "source": [
    "# Create a SparkSession and assign it to variable 'spark'\n",
    "# There are different variants on the usage - refer to the documentation or a tutorial\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"test\") \\\n",
    "    .getOrCreate()\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "ce864ecb",
   "metadata": {},
   "source": [
    "# DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56609aa0",
   "metadata": {},
   "source": [
    "## DataFrame: create manually"
   ]
  },
  {
   "cell_type": "code",
   "id": "974011f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T22:32:43.780971100Z",
     "start_time": "2026-02-13T22:32:35.892815100Z"
    }
   },
   "source": [
    "# Create a DataFrame with one column called “number” and 10000 rows\n",
    "data = spark.range(1000).toDF(\"number\")\n",
    "\n",
    "# Shows the first 20 rows by default\n",
    "data.show()\n",
    "\n",
    "# Show more or fewer rows N\n",
    "N = 50\n",
    "data.show(N)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "|    10|\n",
      "|    11|\n",
      "|    12|\n",
      "|    13|\n",
      "|    14|\n",
      "|    15|\n",
      "|    16|\n",
      "|    17|\n",
      "|    18|\n",
      "|    19|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "|    10|\n",
      "|    11|\n",
      "|    12|\n",
      "|    13|\n",
      "|    14|\n",
      "|    15|\n",
      "|    16|\n",
      "|    17|\n",
      "|    18|\n",
      "|    19|\n",
      "|    20|\n",
      "|    21|\n",
      "|    22|\n",
      "|    23|\n",
      "|    24|\n",
      "|    25|\n",
      "|    26|\n",
      "|    27|\n",
      "|    28|\n",
      "|    29|\n",
      "|    30|\n",
      "|    31|\n",
      "|    32|\n",
      "|    33|\n",
      "|    34|\n",
      "|    35|\n",
      "|    36|\n",
      "|    37|\n",
      "|    38|\n",
      "|    39|\n",
      "|    40|\n",
      "|    41|\n",
      "|    42|\n",
      "|    43|\n",
      "|    44|\n",
      "|    45|\n",
      "|    46|\n",
      "|    47|\n",
      "|    48|\n",
      "|    49|\n",
      "+------+\n",
      "only showing top 50 rows\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "55ac6cc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T22:32:54.039195300Z",
     "start_time": "2026-02-13T22:32:43.933840100Z"
    }
   },
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Python list containing two rows\n",
    "emp = [Row(\"Jack\", 24), Row(\"Bobby\", 26)]\n",
    "\n",
    "# Convert Python data into Spark DataFrame\n",
    "emp_df = spark.createDataFrame(emp, [\"name\",\"age\"])\n",
    "\n",
    "emp_df.show()\n"
   ],
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o56.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (Monster executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:678)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:663)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1034)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.SocketChannelImpl.throwConnectionReset(SocketChannelImpl.java:394)\r\n\tat java.base/sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:426)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:837)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:381)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1022)\r\n\t... 26 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\r\n\tat scala.collection.immutable.List.foreach(List.scala:323)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1017)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2496)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2517)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2536)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\r\n\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2275)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1401)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2265)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2263)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:177)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:285)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:139)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:139)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:308)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:138)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:92)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:250)\r\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2263)\r\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1401)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2814)\r\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:338)\r\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:374)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:678)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:663)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1034)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.SocketChannelImpl.throwConnectionReset(SocketChannelImpl.java:394)\r\n\tat java.base/sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:426)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:837)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:381)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1022)\r\n\t... 26 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mPy4JJavaError\u001B[39m                             Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 9\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[38;5;66;03m# Convert Python data into Spark DataFrame\u001B[39;00m\n\u001B[32m      7\u001B[39m emp_df = spark.createDataFrame(emp, [\u001B[33m\"\u001B[39m\u001B[33mname\u001B[39m\u001B[33m\"\u001B[39m,\u001B[33m\"\u001B[39m\u001B[33mage\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m \u001B[43memp_df\u001B[49m\u001B[43m.\u001B[49m\u001B[43mshow\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\cs5052-practical1\\.venv\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:285\u001B[39m, in \u001B[36mDataFrame.show\u001B[39m\u001B[34m(self, n, truncate, vertical)\u001B[39m\n\u001B[32m    284\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mshow\u001B[39m(\u001B[38;5;28mself\u001B[39m, n: \u001B[38;5;28mint\u001B[39m = \u001B[32m20\u001B[39m, truncate: Union[\u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mint\u001B[39m] = \u001B[38;5;28;01mTrue\u001B[39;00m, vertical: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mFalse\u001B[39;00m) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m285\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_show_string\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtruncate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\cs5052-practical1\\.venv\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:303\u001B[39m, in \u001B[36mDataFrame._show_string\u001B[39m\u001B[34m(self, n, truncate, vertical)\u001B[39m\n\u001B[32m    297\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[32m    298\u001B[39m         errorClass=\u001B[33m\"\u001B[39m\u001B[33mNOT_BOOL\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    299\u001B[39m         messageParameters={\u001B[33m\"\u001B[39m\u001B[33marg_name\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mvertical\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33marg_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(vertical).\u001B[34m__name__\u001B[39m},\n\u001B[32m    300\u001B[39m     )\n\u001B[32m    302\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(truncate, \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m truncate:\n\u001B[32m--> \u001B[39m\u001B[32m303\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_jdf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    304\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    305\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\cs5052-practical1\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001B[39m, in \u001B[36mJavaMember.__call__\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m   1356\u001B[39m command = proto.CALL_COMMAND_NAME +\\\n\u001B[32m   1357\u001B[39m     \u001B[38;5;28mself\u001B[39m.command_header +\\\n\u001B[32m   1358\u001B[39m     args_command +\\\n\u001B[32m   1359\u001B[39m     proto.END_COMMAND_PART\n\u001B[32m   1361\u001B[39m answer = \u001B[38;5;28mself\u001B[39m.gateway_client.send_command(command)\n\u001B[32m-> \u001B[39m\u001B[32m1362\u001B[39m return_value = \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1363\u001B[39m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1365\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[32m   1366\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[33m\"\u001B[39m\u001B[33m_detach\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\cs5052-practical1\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:263\u001B[39m, in \u001B[36mcapture_sql_exception.<locals>.deco\u001B[39m\u001B[34m(*a, **kw)\u001B[39m\n\u001B[32m    260\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpy4j\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mprotocol\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Py4JJavaError\n\u001B[32m    262\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m263\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    264\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    265\u001B[39m     converted = convert_exception(e.java_exception)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\cs5052-practical1\\.venv\\Lib\\site-packages\\py4j\\protocol.py:327\u001B[39m, in \u001B[36mget_return_value\u001B[39m\u001B[34m(answer, gateway_client, target_id, name)\u001B[39m\n\u001B[32m    325\u001B[39m value = OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[32m2\u001B[39m:], gateway_client)\n\u001B[32m    326\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[32m1\u001B[39m] == REFERENCE_TYPE:\n\u001B[32m--> \u001B[39m\u001B[32m327\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[32m    328\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m.\n\u001B[32m    329\u001B[39m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m, name), value)\n\u001B[32m    330\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    331\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[32m    332\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m.\n\u001B[32m    333\u001B[39m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m, name, value))\n",
      "\u001B[31mPy4JJavaError\u001B[39m: An error occurred while calling o56.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (Monster executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:678)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:663)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1034)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.SocketChannelImpl.throwConnectionReset(SocketChannelImpl.java:394)\r\n\tat java.base/sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:426)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:837)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:381)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1022)\r\n\t... 26 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\r\n\tat scala.collection.immutable.List.foreach(List.scala:323)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1017)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2496)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2517)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2536)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\r\n\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2275)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1401)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2265)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2263)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:177)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:285)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:139)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:139)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:308)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:138)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:92)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:250)\r\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2263)\r\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1401)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2814)\r\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:338)\r\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:374)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed). Consider setting 'spark.sql.execution.pyspark.udf.faulthandler.enabled' or'spark.python.worker.faulthandler.enabled' configuration to 'true' for the better Python traceback.\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:678)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:663)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1034)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1014)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.SocketChannelImpl.throwConnectionReset(SocketChannelImpl.java:394)\r\n\tat java.base/sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:426)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:837)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:381)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:1022)\r\n\t... 26 more\r\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "7750b44d",
   "metadata": {},
   "source": [
    "## DataFrame: create from file"
   ]
  },
  {
   "cell_type": "code",
   "id": "9b4cf528",
   "metadata": {},
   "source": [
    "# Create DataFrame from a file\n",
    "df = ( \n",
    "    spark.read\n",
    "    .option(\"header\", True)         # Tells Spark the first line is a header\n",
    "    .option(\"inferSchema\", True)    # Spark scans the column and infers data type; id will be an integer, country and capital will be a string\n",
    "    .format(\"csv\")                  \n",
    "    .load(\"sample_data1.csv\")\n",
    ")\n",
    "\n",
    "df.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a2e8fe5a",
   "metadata": {},
   "source": [
    "## DataFrame: Datasource\n",
    "\n",
    "Many different file types are possible, including CSV, JSON, ORC, Parquet, Text, Table, JDBC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef0ceb4",
   "metadata": {},
   "source": [
    "# Two Major Operations\n",
    "\n",
    "- All abstractions such as RDD and DataFrames offer two types of operation\n",
    "    - **Transformation:** construct a new RDD/DataFrame from a previous one\n",
    "    - **Action:** compute the result based on an RDD/DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93597aad",
   "metadata": {},
   "source": [
    "# Transformations\n",
    "\n",
    "## Transformations: `printSchema()` and `describe()`"
   ]
  },
  {
   "cell_type": "code",
   "id": "64d12110",
   "metadata": {},
   "source": [
    "# Print the structure (data type of the columns) of the DataFrame\n",
    "df.printSchema()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ee06ab7a",
   "metadata": {},
   "source": [
    "# Describes the schema of the DataFrame\n",
    "df.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e4517cce",
   "metadata": {},
   "source": [
    "# Describe the structure of specific column\n",
    "df.select(\"Country\").describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e3dc155b",
   "metadata": {},
   "source": [
    "## Transformations: `where()` and `filter()`"
   ]
  },
  {
   "cell_type": "code",
   "id": "20c958ae",
   "metadata": {},
   "source": [
    "df_population = ( \n",
    "                    spark.read\n",
    "                    .option(\"header\", True)         \n",
    "                    .option(\"inferSchema\", True) \n",
    "                    .format(\"csv\")                  \n",
    "                    .load(\"sample_data2.csv\")\n",
    ")\n",
    "\n",
    "df_population.show()\n",
    "\n",
    "hundredK_plus = df_population.filter(\"Population >= 100000\")\n",
    "hundredK_plus.show()\n",
    "\n",
    "under_50K = df_population.where(\"Population <= 50000\")\n",
    "under_50K.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ce951e13",
   "metadata": {},
   "source": [
    "## Transformation: `distinct()` and\t`limit()` "
   ]
  },
  {
   "cell_type": "code",
   "id": "e95a04f1",
   "metadata": {},
   "source": [
    "df_town_village = ( \n",
    "                    spark.read\n",
    "                    .option(\"header\", True)         \n",
    "                    .option(\"inferSchema\", True) \n",
    "                    .format(\"csv\")                  \n",
    "                    .load(\"sample_data3.csv\")\n",
    ")\n",
    "\n",
    "df_town_village.show()\n",
    "\n",
    "unique_county = df_town_village.select(\"County\").distinct()\n",
    "unique_county.show()\n",
    "\n",
    "N = 5\n",
    "shortN_list = df_town_village.limit(N)\n",
    "shortN_list.show()\n",
    "\n",
    "# Alternate usage\n",
    "df_town_village.limit(N).show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "89a3dc99",
   "metadata": {},
   "source": [
    "## Transformation: Sorting using `sort()` or `orderBy()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992b324c",
   "metadata": {},
   "source": [
    "### Basic sorting"
   ]
  },
  {
   "cell_type": "code",
   "id": "5ab48fd6",
   "metadata": {},
   "source": [
    "# Sort by a single column\n",
    "sorted = df_town_village.sort(\"County\")\n",
    "sorted.show()\n",
    "\n",
    "# Sort by multiple columns\n",
    "sorted = df_town_village.sort(\"County\", \"Town/Village\")\n",
    "sorted.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "815e9fe0",
   "metadata": {},
   "source": [
    "# Order by a single column\n",
    "sorted = df_town_village.orderBy(\"Town/Village\")\n",
    "sorted.show()\n",
    "\n",
    "# Order by multiple columns\n",
    "sorted = df_town_village.orderBy(\"County\", \"Town/Village\")\n",
    "sorted.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5e35e3a3",
   "metadata": {},
   "source": [
    "### Specifying sort direction"
   ]
  },
  {
   "cell_type": "code",
   "id": "fafa0363",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import desc, asc \n",
    "\n",
    "sorted = df_town_village.orderBy(desc(\"Town/Village\"))\n",
    "sorted.show()\n",
    "\n",
    "sorted = df_town_village.orderBy(asc(\"County\"), desc(\"Town/Village\"))\n",
    "sorted.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "170dd105",
   "metadata": {},
   "source": [
    "## Transformation: Sampling data using `sample`"
   ]
  },
  {
   "cell_type": "code",
   "id": "d3bef7a9",
   "metadata": {},
   "source": [
    "with_replacement = False    # Sample without replacement; each row can appear at most once \n",
    "fraction = 0.50             # Roughly 50% of the rows are selected\n",
    "seed = None                 # Sets the random seed for reproducibility; if an integer sample value is set it produces the same sample everytime \n",
    "\n",
    "sample = df_town_village.sample(with_replacement, fraction, seed)\n",
    "sample.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "738e0474",
   "metadata": {},
   "source": [
    "## Transformation: Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "id": "e4d49bf2",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import count, countDistinct \n",
    "\n",
    "df_town_village.select(count(\"County\")).show()\n",
    "\n",
    "df_town_village.select(countDistinct(\"County\")).show()\n",
    "\n",
    "# min, max, avg, first, last and groupBy functions are available and self explanatory\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7114d6da",
   "metadata": {},
   "source": [
    "## DataFrame: Some Actions"
   ]
  },
  {
   "cell_type": "code",
   "id": "3458c7e1",
   "metadata": {},
   "source": [
    "# first()\n",
    "row = df_town_village.first()\n",
    "print(row)\n",
    "print(row[\"Town/Village\"])      #Access column of the first row\n",
    "\n",
    "# show()\n",
    "df_town_village.show()\n",
    "N = 6\n",
    "df_town_village.show(N)\n",
    "\n",
    "# take(N)\n",
    "N = 4\n",
    "rows = df_town_village.take(N)  #Similar to first, but returns multiple rows\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# collect()\n",
    "all_rows = df_town_village.collect()    #Returns all rows as a list of objects\n",
    "# Note: if the DataFrame is large, then may not work as all memory is brought into memory\n",
    "# Use this for small datasets or debugging\n",
    "for row in all_rows:\n",
    "    print(row)\n",
    "\n",
    "#count()\n",
    "print(f\"Total rows: {df_town_village.count()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8861f3f1",
   "metadata": {},
   "source": [
    "# RDD\n",
    "\n",
    "- Low level but still relevant in some cases:\n",
    "- Raw data processing e.g. text file without structure.\n",
    "    - Creating new RDDs\n",
    "    - Transforming existing RDDs\n",
    "    - Computing results from RDDs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fe500b",
   "metadata": {},
   "source": [
    "## Create RDD"
   ]
  },
  {
   "cell_type": "code",
   "id": "846fcfe1",
   "metadata": {},
   "source": [
    "# From an existing file\n",
    "lines = spark.sparkContext.textFile(\"README_dummy.md\")\n",
    "\n",
    "# or\n",
    "sc = spark.sparkContext\n",
    "lines = sc.textFile(\"README_dummy.md\")\n",
    "\n",
    "# Collect all lines into a Python list\n",
    "all_lines = lines.collect()\n",
    "\n",
    "# Print each line\n",
    "for line in all_lines:\n",
    "    print(line)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8a299b4d",
   "metadata": {},
   "source": [
    "# From a list\n",
    "numbers = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "# numbers is an RDD containing numbers 1 to 8\n",
    "# The data is split into partitions and can be processed in parallel\n",
    "\n",
    "# Aggregate the partitions and print all numbers\n",
    "print(numbers.collect())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "94f78071",
   "metadata": {},
   "source": [
    "# Create an RDD from a file \n",
    "lines = sc.textFile(\"README_dummy.md\")\n",
    "\n",
    "# Create new RDD with lines containing Spark\n",
    "lines = lines.filter(lambda x: 'Spark' in x)\n",
    "\n",
    "# Count the number of items in this RDD\n",
    "# Note: The above two lines doesn't do anything \n",
    "# The statement below will read the file and do the computation\n",
    "print(lines.count())\n",
    "\n",
    "# The statement below will read the file and do the computation again\n",
    "print(lines.count())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "200690c4",
   "metadata": {},
   "source": [
    "## RDD - Persisting\n",
    "\n",
    "- Spark recomputes RDDs each time an action is performed on it\n",
    "    - By default RDD is not stored in memory\n",
    "    - The `persist()` function stores an RDD permanently"
   ]
  },
  {
   "cell_type": "code",
   "id": "d479898f",
   "metadata": {},
   "source": [
    "lines = sc.textFile(\"README_dummy.md\")\n",
    "lines = lines.filter(lambda x: 'Spark' in x)\n",
    "\n",
    "# Load and store dataset in memory\n",
    "lines.persist()\n",
    "\n",
    "# Perform computation on the stored dataset\n",
    "print(lines.count())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b0cb9faf",
   "metadata": {},
   "source": [
    "## Basic RDD Transformation Functions\n",
    "\n",
    "- Construct an RDD from a previous one\n",
    "    - Performed on one or more RDDs\n",
    "    - Return a new RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74bfbfc",
   "metadata": {},
   "source": [
    "### `filter()`\n",
    "- Takes in a function, returns an RDD that only has elements that pass the filter() function"
   ]
  },
  {
   "cell_type": "code",
   "id": "a1a6e708",
   "metadata": {},
   "source": [
    "lines = sc.textFile(\"README_dummy.md\")\n",
    "\n",
    "# Create a new RDD consisting lines that contain ‘Spark’\n",
    "lines = lines.filter(lambda x: 'Spark' in x)\n",
    "\n",
    "all_lines = lines.collect()\n",
    "\n",
    "for line in all_lines:\n",
    "    print(line)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e7c319b1",
   "metadata": {},
   "source": [
    "### `map()`\n",
    "- Takes in a function and applies it to each element in the RDD"
   ]
  },
  {
   "cell_type": "code",
   "id": "3b415d0a",
   "metadata": {},
   "source": [
    "lines = sc.textFile(\"README_dummy.md\")\n",
    "\n",
    "# Create a new RDD in which all strings are in uppercase\n",
    "lines = lines.map(lambda x: x.upper())\n",
    "\n",
    "all_lines = lines.collect()\n",
    "\n",
    "for line in all_lines:\n",
    "    print(line)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "99285319",
   "metadata": {},
   "source": [
    "### `flatmap()`\n",
    "- Applies a function to each element in an RDD\n",
    "- Returns a sequence (list of elements)\n",
    "- The final RDD is flattened"
   ]
  },
  {
   "cell_type": "code",
   "id": "d5cbc518",
   "metadata": {},
   "source": [
    "lines = sc.parallelize([\n",
    "    \"I love Spark\",\n",
    "    \"Spark is awesome\",\n",
    "    \"Big data rocks\"\n",
    "])\n",
    "\n",
    "words_using_map = lines.map(lambda line: line.split(\" \"))\n",
    "print(words_using_map.collect())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "26194aad",
   "metadata": {},
   "source": [
    "words_using_flatmap = lines.flatMap(lambda line: line.split(\" \"))\n",
    "print(words_using_flatmap.collect())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f03683f6",
   "metadata": {},
   "source": [
    "### `distinct()`\n",
    "- Returns a new RDD with only distinct items"
   ]
  },
  {
   "cell_type": "code",
   "id": "21054f0a",
   "metadata": {},
   "source": [
    "numbers = sc.parallelize([0, 1, 2, 4, 7, 5, 4, 3, 2, 1, 1, 0])\n",
    "numbers = numbers.distinct()\n",
    "print(numbers.collect())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e9720738",
   "metadata": {},
   "source": [
    "### `union(other)`\n",
    "- Returns a new RDD consisting of items from both sources"
   ]
  },
  {
   "cell_type": "code",
   "id": "3196c6eb",
   "metadata": {},
   "source": [
    "numbers = sc.parallelize([0, 1, 2, 3, 4])\n",
    "characters = sc.parallelize(['A', 'B', 'C', 'D', 'E'])\n",
    "result = numbers.union(characters)\n",
    "print(result.collect())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e22e8ea3",
   "metadata": {},
   "source": [
    "### `intersection(other)`\n",
    "- Returns a new RDD consisting of only items from both sources and removes all duplicates"
   ]
  },
  {
   "cell_type": "code",
   "id": "4c6bc156",
   "metadata": {},
   "source": [
    "number_list1 = sc.parallelize([0, 1, 2, 4, 6, 7, 8])\n",
    "number_list2 = sc.parallelize([0, 1, 3, 4, 5, 7])\n",
    "result = number_list1.intersection(number_list2)\n",
    "print(result.collect())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d14cf11e",
   "metadata": {},
   "source": [
    "### `subtract(other)`\n",
    "- Returns a new RDD consisting of only items in the first RDD but not in the other one"
   ]
  },
  {
   "cell_type": "code",
   "id": "4fb18594",
   "metadata": {},
   "source": [
    "number_list1 = sc.parallelize([0, 1, 2, 4, 6, 7, 8])\n",
    "number_list2 = sc.parallelize([0, 1, 3, 4, 5, 7])\n",
    "result = number_list1.subtract(number_list2)\n",
    "print(result.collect())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4f688102",
   "metadata": {},
   "source": [
    "## Basic RDD Action Functions\n",
    "\n",
    "- Compute result based on RDD(s)\n",
    "    - Performed on one or more RDD(s)\n",
    "    - Return a result, which is not an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aaf4e5",
   "metadata": {},
   "source": [
    "### `first()`\n",
    "- Returns the first item in an RDD"
   ]
  },
  {
   "cell_type": "code",
   "id": "9f8ac995",
   "metadata": {},
   "source": [
    "numbers = sc.parallelize([0, 1, 2, 3, 4, 5, 6, 7, 8]) \n",
    "print(numbers.first())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3b64e2b3",
   "metadata": {},
   "source": [
    "### `collect()`\n",
    "- Returns a list containing the entire RDD's content"
   ]
  },
  {
   "cell_type": "code",
   "id": "2be6d975",
   "metadata": {},
   "source": [
    "numbers = sc.parallelize([0, 1, 2, 3, 4, 5, 6, 7, 8]) \n",
    "print(numbers.collect())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "95c7cdb2",
   "metadata": {},
   "source": [
    "### `count()`\n",
    "- Returns the number of items in an RDD"
   ]
  },
  {
   "cell_type": "code",
   "id": "e86dcef1",
   "metadata": {},
   "source": [
    "numbers = sc.parallelize([0, 1, 2, 3, 4, 5, 6, 7, 8]) \n",
    "print(numbers.count())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "47196a9e",
   "metadata": {},
   "source": [
    "### `reduce(function)`\n",
    "- Takes a function that operates on two elements and returns a new element"
   ]
  },
  {
   "cell_type": "code",
   "id": "0c9548b4",
   "metadata": {},
   "source": [
    "numbers = sc.parallelize([1, 2, 3, 4, 5]) \n",
    "result = numbers.reduce(lambda x, y: x * y)\n",
    "print(result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "be774f67",
   "metadata": {},
   "source": [
    "### `takeOrdered(num, ordering)`\n",
    "- Return a number of items based on the provided ordering"
   ]
  },
  {
   "cell_type": "code",
   "id": "a08f1183",
   "metadata": {},
   "source": [
    "numbers = sc.parallelize([8, 0, 4, 6, 9, 7, 2, 1, 5, 3])\n",
    "\n",
    "# Return five smallest numbers from the list\n",
    "print(numbers.takeOrdered(5, lambda x: x))\n",
    "\n",
    "# Return five largest numbers from the list\n",
    "print(numbers.takeOrdered(5, lambda x: -x))\n",
    "# Note: How this function works:\n",
    "# Original numbers: 8, 0, 4, 6, 9, 7, 2, 1, 5, 3\n",
    "# Negated numbers: -8, 0, -4, -6, -9, -7, -2, -1, -5, -3\n",
    "# 5 smallest of these: -9, -8, -7, -6, -5\n",
    "# Negate back: 9, 8, 7, 6, 5"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
